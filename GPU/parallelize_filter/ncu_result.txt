==PROF== Connected to process 2785226 (/home/jiahuaz/ChangeDetection/parallelize_filter/main)
==PROF== Profiling "Pearson" - 0: 0%....50%....100% - 11 passes
==PROF== Profiling "Add(double *, double *, double)" - 1: 0%....50%....100% - 11 passes
==PROF== Profiling "SpatialFilter" - 2: 0%....50%....100% - 11 passes
==PROF== Profiling "Pearson" - 3: 0%....50%....100% - 11 passes
==PROF== Profiling "Pearson" - 4: 0%....50%....100% - 11 passes
==PROF== Profiling "Add(double *, double *, double)" - 5: 0%....50%....100% - 11 passes
==PROF== Profiling "SpatialFilter" - 6: 0%....50%....100% - 11 passes
==PROF== Profiling "Pearson" - 7: 0%....50%....100% - 11 passes
==PROF== Profiling "Pearson" - 8: 0%....50%....100% - 11 passes
==PROF== Profiling "Add(double *, double *, double)" - 9: 0%....50%....100% - 11 passes
==PROF== Profiling "SpatialFilter" - 10: 0%....50%....100% - 11 passes
==PROF== Profiling "Pearson" - 11: 0%....50%....100% - 11 passes
==PROF== Profiling "Pearson" - 12: 0%....50%....100% - 11 passes
==PROF== Profiling "Add(double *, double *, double)" - 13: 0%....50%....100% - 11 passes
==PROF== Profiling "SpatialFilter" - 14: 0%....50%....100% - 11 passes
==PROF== Profiling "Pearson" - 15: 0%....50%....100% - 11 passes
==PROF== Profiling "Pearson" - 16: 0%....50%....100% - 11 passes
==PROF== Profiling "Add(double *, double *, double)" - 17: 0%....50%....100% - 11 passes
==PROF== Profiling "SpatialFilter" - 18: 0%....50%....100% - 11 passes
==PROF== Profiling "Pearson" - 19: 0%....50%....100% - 11 passes
==PROF== Profiling "Pearson" - 20: 0%....50%....100% - 11 passes
==PROF== Profiling "Add(double *, double *, double)" - 21: 0%....50%....100% - 11 passes
==PROF== Profiling "SpatialFilter" - 22: 0%....50%....100% - 11 passes
==PROF== Profiling "Pearson" - 23: 0%....50%....100% - 11 passes
==PROF== Profiling "Pearson" - 24: 0%....50%....100% - 11 passes
==PROF== Profiling "Add(double *, double *, double)" - 25: 0%....50%....100% - 11 passes
==PROF== Profiling "SpatialFilter" - 26: 0%....50%....100% - 11 passes
==PROF== Profiling "Pearson" - 27: 0%....50%....100% - 11 passes
==PROF== Profiling "Pearson" - 28: 0%....50%....100% - 11 passes
==PROF== Profiling "Add(double *, double *, double)" - 29: 0%....50%....100% - 11 passes
==PROF== Profiling "SpatialFilter" - 30: 0%....50%....100% - 11 passes
==PROF== Profiling "Pearson" - 31: 0%....50%....100% - 11 passes
==PROF== Profiling "Pearson" - 32: 0%....50%....100% - 11 passes
==PROF== Profiling "Add(double *, double *, double)" - 33: 0%....50%....100% - 11 passes
==PROF== Profiling "SpatialFilter" - 34: 0%....50%....100% - 11 passes
==PROF== Profiling "Pearson" - 35: 0%....50%....100% - 11 passes
==PROF== Profiling "Pearson" - 36: 0%....50%....100% - 11 passes
==PROF== Profiling "Add(double *, double *, double)" - 37: 0%....50%....100% - 11 passes
==PROF== Profiling "SpatialFilter" - 38: 0%....50%....100% - 11 passes
==PROF== Profiling "Pearson" - 39: 0%....50%....100% - 11 passes
==PROF== Profiling "Pearson" - 40: 0%....50%....100% - 11 passes
==PROF== Profiling "Add(double *, double *, double)" - 41: 0%....50%....100% - 11 passes
==PROF== Profiling "SpatialFilter" - 42: 0%....50%....100% - 11 passes
==PROF== Profiling "Pearson" - 43: 0%....50%....100% - 11 passes
==PROF== Profiling "Pearson" - 44: 0%....50%....100% - 11 passes
==PROF== Profiling "Add(double *, double *, double)" - 45: 0%....50%....100% - 11 passes
==PROF== Profiling "SpatialFilter" - 46: 0%....50%....100% - 11 passes
==PROF== Profiling "Pearson" - 47: 0%....50%....100% - 11 passes
==PROF== Profiling "Pearson" - 48: 0%....50%....100% - 11 passes
==PROF== Profiling "Add(double *, double *, double)" - 49: 0%....50%....100% - 11 passes
==PROF== Profiling "SpatialFilter" - 50: 0%....50%....100% - 11 passes
==PROF== Profiling "Pearson" - 51: 0%....50%....100% - 11 passes
==PROF== Profiling "Pearson" - 52: 0%....50%....100% - 11 passes
==PROF== Profiling "Add(double *, double *, double)" - 53: 0%....50%....100% - 11 passes
==PROF== Profiling "SpatialFilter" - 54: 0%....50%....100% - 11 passes
==PROF== Profiling "Pearson" - 55: 0%....50%....100% - 11 passes
==PROF== Profiling "Pearson" - 56: 0%....50%....100% - 11 passes
==PROF== Profiling "Add(double *, double *, double)" - 57: 0%....50%....100% - 11 passes
==PROF== Profiling "SpatialFilter" - 58: 0%....50%....100% - 11 passes
==PROF== Profiling "Pearson" - 59: 0%....50%....100% - 11 passes
==PROF== Profiling "Pearson" - 60: 0%....50%....100% - 11 passes
==PROF== Profiling "Add(double *, double *, double)" - 61: 0%....50%....100% - 11 passes
==PROF== Profiling "SpatialFilter" - 62: 0%....50%....100% - 11 passes
==PROF== Profiling "Pearson" - 63: 0%....50%....100% - 11 passes
==PROF== Profiling "Pearson" - 64: 0%....50%....100% - 11 passes
==PROF== Profiling "Add(double *, double *, double)" - 65: 0%....50%....100% - 11 passes
==PROF== Profiling "SpatialFilter" - 66: 0%....50%....100% - 11 passes
==PROF== Profiling "Pearson" - 67: 0%....50%....100% - 11 passes
==PROF== Profiling "Pearson" - 68: 0%....50%....100% - 11 passes
==PROF== Profiling "Add(double *, double *, double)" - 69: 0%....50%....100% - 11 passes
==PROF== Profiling "SpatialFilter" - 70: 0%....50%....100% - 11 passes
==PROF== Profiling "Pearson" - 71: 0%....50%....100% - 11 passes
Compute time: 39.267 seconds
==PROF== Disconnected from process 2785226
[2785226] main@127.0.0.1
  Pearson(double *, double *, double *) (1, 1, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         2.62
    SM Frequency            cycle/nsecond         1.53
    Elapsed Cycles                  cycle    2,218,102
    Memory Throughput                   %         0.07
    DRAM Throughput                     %         0.07
    Duration                      msecond         1.45
    L1/TEX Cache Throughput             %         4.53
    L2 Cache Throughput                 %         0.07
    SM Active Cycles                cycle    16,776.87
    Compute (SM) Throughput             %         0.21
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Threads                                   thread             512
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 99.24%                                                                                          
          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 132             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        24.93
    Achieved Active Warps Per SM           warp        15.95
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50.15%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (24.9%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      2,656.83
    Total DRAM Elapsed Cycles        cycle   182,248,704
    Average L1 Active Cycles         cycle     16,776.87
    Total L1 Elapsed Cycles          cycle   292,770,836
    Average L2 Active Cycles         cycle    101,084.57
    Total L2 Elapsed Cycles          cycle   237,721,344
    Average SM Active Cycles         cycle     16,776.87
    Total SM Elapsed Cycles          cycle   292,770,836
    Average SMSP Active Cycles       cycle     16,737.63
    Total SMSP Elapsed Cycles        cycle 1,171,083,344
    -------------------------- ----------- -------------

  Add(double *, double *, double) (128, 1, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         2.62
    SM Frequency            cycle/nsecond         1.52
    Elapsed Cycles                  cycle       20,904
    Memory Throughput                   %         7.29
    DRAM Throughput                     %         7.29
    Duration                      usecond        13.70
    L1/TEX Cache Throughput             %         6.75
    L2 Cache Throughput                 %        14.27
    SM Active Cycles                cycle    13,434.61
    Compute (SM) Throughput             %        14.09
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.2 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Threads                                   thread          65,536
    Uses Green Context                                             0
    Waves Per SM                                                0.24
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 3.03%                                                                                           
          The grid for this launch is configured to execute only 128 blocks, which is less than the GPU's 132           
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit SM                        block           32
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        24.81
    Achieved Active Warps Per SM           warp        15.88
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75.19%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     2,612.67
    Total DRAM Elapsed Cycles        cycle    1,719,808
    Average L1 Active Cycles         cycle    13,434.61
    Total L1 Elapsed Cycles          cycle    2,753,530
    Average L2 Active Cycles         cycle    11,869.82
    Total L2 Elapsed Cycles          cycle    2,227,296
    Average SM Active Cycles         cycle    13,434.61
    Total SM Elapsed Cycles          cycle    2,753,530
    Average SMSP Active Cycles       cycle    13,385.17
    Total SMSP Elapsed Cycles        cycle   11,014,120
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.096%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 7.91% above the average, while the minimum instance value is 100.00% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.711%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 8.90% above the average, while the minimum instance value is 100.00% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.096%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 7.91% above the average, while the minimum instance value is 100.00% below the      
          average.                                                                                                      

  SpatialFilter(double *, double *) (128, 1, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         2.62
    SM Frequency            cycle/nsecond         1.53
    Elapsed Cycles                  cycle      714,703
    Memory Throughput                   %         2.72
    DRAM Throughput                     %         0.11
    Duration                      usecond       467.17
    L1/TEX Cache Throughput             %         2.97
    L2 Cache Throughput                 %         1.31
    SM Active Cycles                cycle   653,696.02
    Compute (SM) Throughput             %        36.37
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 1.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              66
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Threads                                   thread          65,536
    Uses Green Context                                             0
    Waves Per SM                                                0.97
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 3.03%                                                                                           
          The grid for this launch is configured to execute only 128 blocks, which is less than the GPU's 132           
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit SM                        block           32
    Block Limit Registers                 block            1
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        24.59
    Achieved Active Warps Per SM           warp        15.74
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (25.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     1,318.50
    Total DRAM Elapsed Cycles        cycle   58,724,096
    Average L1 Active Cycles         cycle   653,696.02
    Total L1 Elapsed Cycles          cycle   94,319,470
    Average L2 Active Cycles         cycle   147,760.73
    Total L2 Elapsed Cycles          cycle   76,561,344
    Average SM Active Cycles         cycle   653,696.02
    Total SM Elapsed Cycles          cycle   94,319,470
    Average SMSP Active Cycles       cycle   648,183.96
    Total SMSP Elapsed Cycles        cycle  377,277,880
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.253%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 7.93% above the average, while the minimum instance value is 100.00% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.803%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 8.60% above the average, while the minimum instance value is 100.00% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.253%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 7.93% above the average, while the minimum instance value is 100.00% below the      
          average.                                                                                                      

  Pearson(double *, double *, double *) (1, 1, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         2.62
    SM Frequency            cycle/nsecond         1.53
    Elapsed Cycles                  cycle    2,219,714
    Memory Throughput                   %         0.07
    DRAM Throughput                     %         0.07
    Duration                      msecond         1.45
    L1/TEX Cache Throughput             %         4.53
    L2 Cache Throughput                 %         0.08
    SM Active Cycles                cycle    16,784.42
    Compute (SM) Throughput             %         0.21
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Threads                                   thread             512
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 99.24%                                                                                          
          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 132             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        24.93
    Achieved Active Warps Per SM           warp        15.95
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50.15%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (24.9%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      2,618.33
    Total DRAM Elapsed Cycles        cycle   182,382,592
    Average L1 Active Cycles         cycle     16,784.42
    Total L1 Elapsed Cycles          cycle   292,977,296
    Average L2 Active Cycles         cycle    101,960.31
    Total L2 Elapsed Cycles          cycle   237,840,576
    Average SM Active Cycles         cycle     16,784.42
    Total SM Elapsed Cycles          cycle   292,977,296
    Average SMSP Active Cycles       cycle     16,746.46
    Total SMSP Elapsed Cycles        cycle 1,171,909,184
    -------------------------- ----------- -------------

  Pearson(double *, double *, double *) (1, 1, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         2.62
    SM Frequency            cycle/nsecond         1.53
    Elapsed Cycles                  cycle    2,217,308
    Memory Throughput                   %         0.07
    DRAM Throughput                     %         0.07
    Duration                      msecond         1.45
    L1/TEX Cache Throughput             %         4.53
    L2 Cache Throughput                 %         0.08
    SM Active Cycles                cycle    16,764.92
    Compute (SM) Throughput             %         0.21
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Threads                                   thread             512
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 99.24%                                                                                          
          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 132             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        24.93
    Achieved Active Warps Per SM           warp        15.95
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50.15%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (24.9%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      2,618.67
    Total DRAM Elapsed Cycles        cycle   182,182,400
    Average L1 Active Cycles         cycle     16,764.92
    Total L1 Elapsed Cycles          cycle   292,648,260
    Average L2 Active Cycles         cycle    100,126.58
    Total L2 Elapsed Cycles          cycle   237,551,136
    Average SM Active Cycles         cycle     16,764.92
    Total SM Elapsed Cycles          cycle   292,648,260
    Average SMSP Active Cycles       cycle     16,725.78
    Total SMSP Elapsed Cycles        cycle 1,170,593,040
    -------------------------- ----------- -------------

  Add(double *, double *, double) (128, 1, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         2.61
    SM Frequency            cycle/nsecond         1.52
    Elapsed Cycles                  cycle       21,382
    Memory Throughput                   %         7.14
    DRAM Throughput                     %         7.14
    Duration                      usecond        14.02
    L1/TEX Cache Throughput             %         6.75
    L2 Cache Throughput                 %        14.02
    SM Active Cycles                cycle    13,434.92
    Compute (SM) Throughput             %        13.68
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.2 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Threads                                   thread          65,536
    Uses Green Context                                             0
    Waves Per SM                                                0.24
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 3.03%                                                                                           
          The grid for this launch is configured to execute only 128 blocks, which is less than the GPU's 132           
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit SM                        block           32
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        24.72
    Achieved Active Warps Per SM           warp        15.82
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75.28%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     2,612.67
    Total DRAM Elapsed Cycles        cycle    1,756,160
    Average L1 Active Cycles         cycle    13,434.92
    Total L1 Elapsed Cycles          cycle    2,817,466
    Average L2 Active Cycles         cycle    12,363.81
    Total L2 Elapsed Cycles          cycle    2,277,792
    Average SM Active Cycles         cycle    13,434.92
    Total SM Elapsed Cycles          cycle    2,817,466
    Average SMSP Active Cycles       cycle    13,346.54
    Total SMSP Elapsed Cycles        cycle   11,269,864
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.015%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 11.14% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.591%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 10.54% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.015%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 11.14% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  SpatialFilter(double *, double *) (128, 1, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         2.62
    SM Frequency            cycle/nsecond         1.53
    Elapsed Cycles                  cycle      720,410
    Memory Throughput                   %         2.70
    DRAM Throughput                     %         0.11
    Duration                      usecond       470.88
    L1/TEX Cache Throughput             %         2.96
    L2 Cache Throughput                 %         1.30
    SM Active Cycles                cycle   657,165.55
    Compute (SM) Throughput             %        36.28
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 1.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              66
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Threads                                   thread          65,536
    Uses Green Context                                             0
    Waves Per SM                                                0.97
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 3.03%                                                                                           
          The grid for this launch is configured to execute only 128 blocks, which is less than the GPU's 132           
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit SM                        block           32
    Block Limit Registers                 block            1
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        24.59
    Achieved Active Warps Per SM           warp        15.74
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (25.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        1,319
    Total DRAM Elapsed Cycles        cycle   59,191,552
    Average L1 Active Cycles         cycle   657,165.55
    Total L1 Elapsed Cycles          cycle   95,064,436
    Average L2 Active Cycles         cycle   148,701.64
    Total L2 Elapsed Cycles          cycle   77,187,360
    Average SM Active Cycles         cycle   657,165.55
    Total SM Elapsed Cycles          cycle   95,064,436
    Average SMSP Active Cycles       cycle   651,434.66
    Total SMSP Elapsed Cycles        cycle  380,257,744
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.352%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 8.06% above the average, while the minimum instance value is 100.00% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.981%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 8.82% above the average, while the minimum instance value is 100.00% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.352%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 8.06% above the average, while the minimum instance value is 100.00% below the      
          average.                                                                                                      

  Pearson(double *, double *, double *) (1, 1, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         2.62
    SM Frequency            cycle/nsecond         1.53
    Elapsed Cycles                  cycle    2,219,875
    Memory Throughput                   %         0.07
    DRAM Throughput                     %         0.07
    Duration                      msecond         1.45
    L1/TEX Cache Throughput             %         4.53
    L2 Cache Throughput                 %         0.08
    SM Active Cycles                cycle    16,777.59
    Compute (SM) Throughput             %         0.21
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Threads                                   thread             512
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 99.24%                                                                                          
          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 132             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        24.93
    Achieved Active Warps Per SM           warp        15.95
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50.15%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (24.9%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      2,618.33
    Total DRAM Elapsed Cycles        cycle   182,395,648
    Average L1 Active Cycles         cycle     16,777.59
    Total L1 Elapsed Cycles          cycle   293,011,464
    Average L2 Active Cycles         cycle     99,722.36
    Total L2 Elapsed Cycles          cycle   237,779,136
    Average SM Active Cycles         cycle     16,777.59
    Total SM Elapsed Cycles          cycle   293,011,464
    Average SMSP Active Cycles       cycle     16,745.88
    Total SMSP Elapsed Cycles        cycle 1,172,045,856
    -------------------------- ----------- -------------

  Pearson(double *, double *, double *) (1, 1, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         2.62
    SM Frequency            cycle/nsecond         1.53
    Elapsed Cycles                  cycle    2,216,909
    Memory Throughput                   %         0.07
    DRAM Throughput                     %         0.07
    Duration                      msecond         1.45
    L1/TEX Cache Throughput             %         4.53
    L2 Cache Throughput                 %         0.09
    SM Active Cycles                cycle    16,768.44
    Compute (SM) Throughput             %         0.21
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Threads                                   thread             512
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 99.24%                                                                                          
          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 132             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        24.93
    Achieved Active Warps Per SM           warp        15.95
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50.15%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (24.9%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      2,618.17
    Total DRAM Elapsed Cycles        cycle   182,152,832
    Average L1 Active Cycles         cycle     16,768.44
    Total L1 Elapsed Cycles          cycle   292,617,388
    Average L2 Active Cycles         cycle    102,633.65
    Total L2 Elapsed Cycles          cycle   237,532,800
    Average SM Active Cycles         cycle     16,768.44
    Total SM Elapsed Cycles          cycle   292,617,388
    Average SMSP Active Cycles       cycle     16,731.14
    Total SMSP Elapsed Cycles        cycle 1,170,469,552
    -------------------------- ----------- -------------

  Add(double *, double *, double) (128, 1, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         2.61
    SM Frequency            cycle/nsecond         1.52
    Elapsed Cycles                  cycle       20,866
    Memory Throughput                   %         7.32
    DRAM Throughput                     %         7.32
    Duration                      usecond        13.66
    L1/TEX Cache Throughput             %         6.96
    L2 Cache Throughput                 %        14.33
    SM Active Cycles                cycle    13,035.96
    Compute (SM) Throughput             %        13.63
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.2 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Threads                                   thread          65,536
    Uses Green Context                                             0
    Waves Per SM                                                0.24
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 3.03%                                                                                           
          The grid for this launch is configured to execute only 128 blocks, which is less than the GPU's 132           
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit SM                        block           32
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        24.83
    Achieved Active Warps Per SM           warp        15.89
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75.17%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     2,612.67
    Total DRAM Elapsed Cycles        cycle    1,713,408
    Average L1 Active Cycles         cycle    13,035.96
    Total L1 Elapsed Cycles          cycle    2,748,098
    Average L2 Active Cycles         cycle    11,928.75
    Total L2 Elapsed Cycles          cycle    2,221,056
    Average SM Active Cycles         cycle    13,035.96
    Total SM Elapsed Cycles          cycle    2,748,098
    Average SMSP Active Cycles       cycle    13,013.37
    Total SMSP Elapsed Cycles        cycle   10,992,392
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.669%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 10.65% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.608%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 8.97% above the average, while the minimum instance value is 100.00% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.669%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 10.65% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  SpatialFilter(double *, double *) (128, 1, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         2.62
    SM Frequency            cycle/nsecond         1.53
    Elapsed Cycles                  cycle      721,887
    Memory Throughput                   %         2.69
    DRAM Throughput                     %         0.11
    Duration                      usecond       471.87
    L1/TEX Cache Throughput             %         2.95
    L2 Cache Throughput                 %         1.29
    SM Active Cycles                cycle   659,805.80
    Compute (SM) Throughput             %        36.39
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 1.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              66
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Threads                                   thread          65,536
    Uses Green Context                                             0
    Waves Per SM                                                0.97
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 3.03%                                                                                           
          The grid for this launch is configured to execute only 128 blocks, which is less than the GPU's 132           
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit SM                        block           32
    Block Limit Registers                 block            1
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        24.59
    Achieved Active Warps Per SM           warp        15.74
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (25.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     1,316.83
    Total DRAM Elapsed Cycles        cycle   59,314,176
    Average L1 Active Cycles         cycle   659,805.80
    Total L1 Elapsed Cycles          cycle   95,267,096
    Average L2 Active Cycles         cycle   130,775.44
    Total L2 Elapsed Cycles          cycle   77,324,544
    Average SM Active Cycles         cycle   659,805.80
    Total SM Elapsed Cycles          cycle   95,267,096
    Average SMSP Active Cycles       cycle   653,931.88
    Total SMSP Elapsed Cycles        cycle  381,068,384
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.09%                                                                                           
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 7.75% above the average, while the minimum instance value is 100.00% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.792%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 8.60% above the average, while the minimum instance value is 100.00% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.09%                                                                                           
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 7.75% above the average, while the minimum instance value is 100.00% below the      
          average.                                                                                                      

  Pearson(double *, double *, double *) (1, 1, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         2.62
    SM Frequency            cycle/nsecond         1.53
    Elapsed Cycles                  cycle    2,220,323
    Memory Throughput                   %         0.07
    DRAM Throughput                     %         0.07
    Duration                      msecond         1.45
    L1/TEX Cache Throughput             %         4.53
    L2 Cache Throughput                 %         0.07
    SM Active Cycles                cycle    16,780.72
    Compute (SM) Throughput             %         0.21
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Threads                                   thread             512
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 99.24%                                                                                          
          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 132             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        24.93
    Achieved Active Warps Per SM           warp        15.95
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50.15%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (24.9%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      2,618.17
    Total DRAM Elapsed Cycles        cycle   182,429,696
    Average L1 Active Cycles         cycle     16,780.72
    Total L1 Elapsed Cycles          cycle   293,033,732
    Average L2 Active Cycles         cycle    102,398.31
    Total L2 Elapsed Cycles          cycle   237,841,824
    Average SM Active Cycles         cycle     16,780.72
    Total SM Elapsed Cycles          cycle   293,033,732
    Average SMSP Active Cycles       cycle     16,754.32
    Total SMSP Elapsed Cycles        cycle 1,172,134,928
    -------------------------- ----------- -------------

  Pearson(double *, double *, double *) (1, 1, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         2.62
    SM Frequency            cycle/nsecond         1.53
    Elapsed Cycles                  cycle    2,216,522
    Memory Throughput                   %         0.07
    DRAM Throughput                     %         0.07
    Duration                      msecond         1.45
    L1/TEX Cache Throughput             %         4.53
    L2 Cache Throughput                 %         0.10
    SM Active Cycles                cycle    16,761.85
    Compute (SM) Throughput             %         0.21
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Threads                                   thread             512
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 99.24%                                                                                          
          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 132             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        24.93
    Achieved Active Warps Per SM           warp        15.95
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50.15%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (24.9%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      2,618.83
    Total DRAM Elapsed Cycles        cycle   182,120,448
    Average L1 Active Cycles         cycle     16,761.85
    Total L1 Elapsed Cycles          cycle   292,555,780
    Average L2 Active Cycles         cycle     96,782.69
    Total L2 Elapsed Cycles          cycle   237,370,848
    Average SM Active Cycles         cycle     16,761.85
    Total SM Elapsed Cycles          cycle   292,555,780
    Average SMSP Active Cycles       cycle     16,733.32
    Total SMSP Elapsed Cycles        cycle 1,170,223,120
    -------------------------- ----------- -------------

  Add(double *, double *, double) (128, 1, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         2.62
    SM Frequency            cycle/nsecond         1.52
    Elapsed Cycles                  cycle       20,923
    Memory Throughput                   %         7.29
    DRAM Throughput                     %         7.29
    Duration                      usecond        13.70
    L1/TEX Cache Throughput             %         6.74
    L2 Cache Throughput                 %        14.18
    SM Active Cycles                cycle    13,457.59
    Compute (SM) Throughput             %        13.93
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.2 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Threads                                   thread          65,536
    Uses Green Context                                             0
    Waves Per SM                                                0.24
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 3.03%                                                                                           
          The grid for this launch is configured to execute only 128 blocks, which is less than the GPU's 132           
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit SM                        block           32
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        24.74
    Achieved Active Warps Per SM           warp        15.84
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75.26%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     2,612.67
    Total DRAM Elapsed Cycles        cycle    1,719,808
    Average L1 Active Cycles         cycle    13,457.59
    Total L1 Elapsed Cycles          cycle    2,756,352
    Average L2 Active Cycles         cycle    12,290.22
    Total L2 Elapsed Cycles          cycle    2,226,336
    Average SM Active Cycles         cycle    13,457.59
    Total SM Elapsed Cycles          cycle    2,756,352
    Average SMSP Active Cycles       cycle    13,280.51
    Total SMSP Elapsed Cycles        cycle   11,025,408
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.41%                                                                                           
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 9.95% above the average, while the minimum instance value is 100.00% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.012%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 9.45% above the average, while the minimum instance value is 100.00% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.41%                                                                                           
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 9.95% above the average, while the minimum instance value is 100.00% below the      
          average.                                                                                                      

  SpatialFilter(double *, double *) (128, 1, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         2.62
    SM Frequency            cycle/nsecond         1.53
    Elapsed Cycles                  cycle      721,819
    Memory Throughput                   %         2.69
    DRAM Throughput                     %         0.11
    Duration                      usecond       471.81
    L1/TEX Cache Throughput             %         2.95
    L2 Cache Throughput                 %         1.30
    SM Active Cycles                cycle   658,628.58
    Compute (SM) Throughput             %        36.27
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 1.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              66
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Threads                                   thread          65,536
    Uses Green Context                                             0
    Waves Per SM                                                0.97
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 3.03%                                                                                           
          The grid for this launch is configured to execute only 128 blocks, which is less than the GPU's 132           
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit SM                        block           32
    Block Limit Registers                 block            1
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        24.58
    Achieved Active Warps Per SM           warp        15.73
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (25.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     1,320.17
    Total DRAM Elapsed Cycles        cycle   59,307,776
    Average L1 Active Cycles         cycle   658,628.58
    Total L1 Elapsed Cycles          cycle   95,262,816
    Average L2 Active Cycles         cycle   145,467.92
    Total L2 Elapsed Cycles          cycle   77,348,256
    Average SM Active Cycles         cycle   658,628.58
    Total SM Elapsed Cycles          cycle   95,262,816
    Average SMSP Active Cycles       cycle   652,667.88
    Total SMSP Elapsed Cycles        cycle  381,051,264
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.318%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 8.02% above the average, while the minimum instance value is 100.00% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.997%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 8.84% above the average, while the minimum instance value is 100.00% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.318%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 8.02% above the average, while the minimum instance value is 100.00% below the      
          average.                                                                                                      

  Pearson(double *, double *, double *) (1, 1, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         2.62
    SM Frequency            cycle/nsecond         1.53
    Elapsed Cycles                  cycle    2,218,855
    Memory Throughput                   %         0.07
    DRAM Throughput                     %         0.07
    Duration                      msecond         1.45
    L1/TEX Cache Throughput             %         4.52
    L2 Cache Throughput                 %         0.06
    SM Active Cycles                cycle    16,791.98
    Compute (SM) Throughput             %         0.21
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Threads                                   thread             512
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 99.24%                                                                                          
          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 132             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        24.93
    Achieved Active Warps Per SM           warp        15.95
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50.15%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (24.9%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      2,618.50
    Total DRAM Elapsed Cycles        cycle   182,311,936
    Average L1 Active Cycles         cycle     16,791.98
    Total L1 Elapsed Cycles          cycle   292,872,800
    Average L2 Active Cycles         cycle    103,168.48
    Total L2 Elapsed Cycles          cycle   237,742,848
    Average SM Active Cycles         cycle     16,791.98
    Total SM Elapsed Cycles          cycle   292,872,800
    Average SMSP Active Cycles       cycle     16,750.05
    Total SMSP Elapsed Cycles        cycle 1,171,491,200
    -------------------------- ----------- -------------

  Pearson(double *, double *, double *) (1, 1, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         2.62
    SM Frequency            cycle/nsecond         1.53
    Elapsed Cycles                  cycle    2,216,905
    Memory Throughput                   %         0.07
    DRAM Throughput                     %         0.07
    Duration                      msecond         1.45
    L1/TEX Cache Throughput             %         4.53
    L2 Cache Throughput                 %         0.08
    SM Active Cycles                cycle    16,764.91
    Compute (SM) Throughput             %         0.21
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Threads                                   thread             512
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 99.24%                                                                                          
          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 132             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        24.93
    Achieved Active Warps Per SM           warp        15.95
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50.15%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (24.9%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      2,620.83
    Total DRAM Elapsed Cycles        cycle   182,151,168
    Average L1 Active Cycles         cycle     16,764.91
    Total L1 Elapsed Cycles          cycle   292,615,366
    Average L2 Active Cycles         cycle    113,610.29
    Total L2 Elapsed Cycles          cycle   237,577,152
    Average SM Active Cycles         cycle     16,764.91
    Total SM Elapsed Cycles          cycle   292,615,366
    Average SMSP Active Cycles       cycle     16,738.92
    Total SMSP Elapsed Cycles        cycle 1,170,461,464
    -------------------------- ----------- -------------

  Add(double *, double *, double) (128, 1, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         2.62
    SM Frequency            cycle/nsecond         1.52
    Elapsed Cycles                  cycle       21,509
    Memory Throughput                   %         7.09
    DRAM Throughput                     %         7.09
    Duration                      usecond        14.08
    L1/TEX Cache Throughput             %         6.94
    L2 Cache Throughput                 %        13.73
    SM Active Cycles                cycle    13,055.55
    Compute (SM) Throughput             %        13.22
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.2 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Threads                                   thread          65,536
    Uses Green Context                                             0
    Waves Per SM                                                0.24
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 3.03%                                                                                           
          The grid for this launch is configured to execute only 128 blocks, which is less than the GPU's 132           
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit SM                        block           32
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        24.84
    Achieved Active Warps Per SM           warp        15.90
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75.16%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     2,612.83
    Total DRAM Elapsed Cycles        cycle    1,767,936
    Average L1 Active Cycles         cycle    13,055.55
    Total L1 Elapsed Cycles          cycle    2,833,926
    Average L2 Active Cycles         cycle    11,909.08
    Total L2 Elapsed Cycles          cycle    2,293,536
    Average SM Active Cycles         cycle    13,055.55
    Total SM Elapsed Cycles          cycle    2,833,926
    Average SMSP Active Cycles       cycle    12,984.63
    Total SMSP Elapsed Cycles        cycle   11,335,704
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.543%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 9.12% above the average, while the minimum instance value is 100.00% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.789%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 12.88% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.543%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 9.12% above the average, while the minimum instance value is 100.00% below the      
          average.                                                                                                      

  SpatialFilter(double *, double *) (128, 1, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         2.62
    SM Frequency            cycle/nsecond         1.53
    Elapsed Cycles                  cycle      720,846
    Memory Throughput                   %         2.70
    DRAM Throughput                     %         0.11
    Duration                      usecond       471.20
    L1/TEX Cache Throughput             %         2.94
    L2 Cache Throughput                 %         1.29
    SM Active Cycles                cycle   659,787.60
    Compute (SM) Throughput             %        36.44
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 1.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              66
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Threads                                   thread          65,536
    Uses Green Context                                             0
    Waves Per SM                                                0.97
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 3.03%                                                                                           
          The grid for this launch is configured to execute only 128 blocks, which is less than the GPU's 132           
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit SM                        block           32
    Block Limit Registers                 block            1
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        24.59
    Achieved Active Warps Per SM           warp        15.74
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (25.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     1,317.83
    Total DRAM Elapsed Cycles        cycle   59,228,416
    Average L1 Active Cycles         cycle   659,787.60
    Total L1 Elapsed Cycles          cycle   95,137,360
    Average L2 Active Cycles         cycle   125,514.41
    Total L2 Elapsed Cycles          cycle   77,238,240
    Average SM Active Cycles         cycle   659,787.60
    Total SM Elapsed Cycles          cycle   95,137,360
    Average SMSP Active Cycles       cycle   653,967.77
    Total SMSP Elapsed Cycles        cycle  380,549,440
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.1%                                                                                            
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 7.76% above the average, while the minimum instance value is 100.00% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.779%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 8.57% above the average, while the minimum instance value is 100.00% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.1%                                                                                            
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 7.76% above the average, while the minimum instance value is 100.00% below the      
          average.                                                                                                      

  Pearson(double *, double *, double *) (1, 1, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         2.62
    SM Frequency            cycle/nsecond         1.53
    Elapsed Cycles                  cycle    2,219,352
    Memory Throughput                   %         0.07
    DRAM Throughput                     %         0.07
    Duration                      msecond         1.45
    L1/TEX Cache Throughput             %         4.53
    L2 Cache Throughput                 %         0.08
    SM Active Cycles                cycle    16,773.68
    Compute (SM) Throughput             %         0.21
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Threads                                   thread             512
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 99.24%                                                                                          
          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 132             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        24.93
    Achieved Active Warps Per SM           warp        15.95
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50.15%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (24.9%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      2,618.83
    Total DRAM Elapsed Cycles        cycle   182,350,592
    Average L1 Active Cycles         cycle     16,773.68
    Total L1 Elapsed Cycles          cycle   292,930,312
    Average L2 Active Cycles         cycle    100,093.64
    Total L2 Elapsed Cycles          cycle   237,739,776
    Average SM Active Cycles         cycle     16,773.68
    Total SM Elapsed Cycles          cycle   292,930,312
    Average SMSP Active Cycles       cycle     16,768.72
    Total SMSP Elapsed Cycles        cycle 1,171,721,248
    -------------------------- ----------- -------------

  Pearson(double *, double *, double *) (1, 1, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         2.62
    SM Frequency            cycle/nsecond         1.53
    Elapsed Cycles                  cycle    2,218,200
    Memory Throughput                   %         0.07
    DRAM Throughput                     %         0.07
    Duration                      msecond         1.45
    L1/TEX Cache Throughput             %         4.52
    L2 Cache Throughput                 %         0.08
    SM Active Cycles                cycle    16,790.73
    Compute (SM) Throughput             %         0.21
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Threads                                   thread             512
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 99.24%                                                                                          
          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 132             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        24.93
    Achieved Active Warps Per SM           warp        15.95
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50.15%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (24.9%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      2,618.50
    Total DRAM Elapsed Cycles        cycle   182,256,128
    Average L1 Active Cycles         cycle     16,790.73
    Total L1 Elapsed Cycles          cycle   292,778,208
    Average L2 Active Cycles         cycle     99,576.89
    Total L2 Elapsed Cycles          cycle   237,643,968
    Average SM Active Cycles         cycle     16,790.73
    Total SM Elapsed Cycles          cycle   292,778,208
    Average SMSP Active Cycles       cycle     16,722.05
    Total SMSP Elapsed Cycles        cycle 1,171,112,832
    -------------------------- ----------- -------------

  Add(double *, double *, double) (128, 1, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         2.62
    SM Frequency            cycle/nsecond         1.52
    Elapsed Cycles                  cycle       21,408
    Memory Throughput                   %         7.13
    DRAM Throughput                     %         7.13
    Duration                      usecond        14.02
    L1/TEX Cache Throughput             %         6.71
    L2 Cache Throughput                 %        13.91
    SM Active Cycles                cycle    13,514.58
    Compute (SM) Throughput             %        13.66
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.2 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Threads                                   thread          65,536
    Uses Green Context                                             0
    Waves Per SM                                                0.24
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 3.03%                                                                                           
          The grid for this launch is configured to execute only 128 blocks, which is less than the GPU's 132           
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit SM                        block           32
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        24.80
    Achieved Active Warps Per SM           warp        15.87
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75.2%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     2,612.83
    Total DRAM Elapsed Cycles        cycle    1,759,872
    Average L1 Active Cycles         cycle    13,514.58
    Total L1 Elapsed Cycles          cycle    2,820,732
    Average L2 Active Cycles         cycle    12,312.03
    Total L2 Elapsed Cycles          cycle    2,280,288
    Average SM Active Cycles         cycle    13,514.58
    Total SM Elapsed Cycles          cycle    2,820,732
    Average SMSP Active Cycles       cycle    13,318.63
    Total SMSP Elapsed Cycles        cycle   11,282,928
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.234%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 8.28% above the average, while the minimum instance value is 100.00% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.51%                                                                                           
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 10.44% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.234%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 8.28% above the average, while the minimum instance value is 100.00% below the      
          average.                                                                                                      

  SpatialFilter(double *, double *) (128, 1, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         2.62
    SM Frequency            cycle/nsecond         1.53
    Elapsed Cycles                  cycle      718,809
    Memory Throughput                   %         2.70
    DRAM Throughput                     %         0.11
    Duration                      usecond       469.86
    L1/TEX Cache Throughput             %         2.95
    L2 Cache Throughput                 %         1.30
    SM Active Cycles                cycle   657,303.52
    Compute (SM) Throughput             %        36.35
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 1.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              66
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Threads                                   thread          65,536
    Uses Green Context                                             0
    Waves Per SM                                                0.97
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 3.03%                                                                                           
          The grid for this launch is configured to execute only 128 blocks, which is less than the GPU's 132           
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit SM                        block           32
    Block Limit Registers                 block            1
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        24.59
    Achieved Active Warps Per SM           warp        15.74
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (25.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        1,318
    Total DRAM Elapsed Cycles        cycle   59,061,504
    Average L1 Active Cycles         cycle   657,303.52
    Total L1 Elapsed Cycles          cycle   94,866,030
    Average L2 Active Cycles         cycle   149,695.12
    Total L2 Elapsed Cycles          cycle   77,001,024
    Average SM Active Cycles         cycle   657,303.52
    Total SM Elapsed Cycles          cycle   94,866,030
    Average SMSP Active Cycles       cycle   651,559.12
    Total SMSP Elapsed Cycles        cycle  379,464,120
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.135%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 7.80% above the average, while the minimum instance value is 100.00% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.803%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 8.61% above the average, while the minimum instance value is 100.00% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.135%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 7.80% above the average, while the minimum instance value is 100.00% below the      
          average.                                                                                                      

  Pearson(double *, double *, double *) (1, 1, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         2.62
    SM Frequency            cycle/nsecond         1.53
    Elapsed Cycles                  cycle    2,219,153
    Memory Throughput                   %         0.07
    DRAM Throughput                     %         0.07
    Duration                      msecond         1.45
    L1/TEX Cache Throughput             %         4.53
    L2 Cache Throughput                 %         0.07
    SM Active Cycles                cycle    16,781.20
    Compute (SM) Throughput             %         0.21
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Threads                                   thread             512
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 99.24%                                                                                          
          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 132             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        24.93
    Achieved Active Warps Per SM           warp        15.95
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50.15%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (24.9%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      2,618.50
    Total DRAM Elapsed Cycles        cycle   182,335,488
    Average L1 Active Cycles         cycle     16,781.20
    Total L1 Elapsed Cycles          cycle   292,909,708
    Average L2 Active Cycles         cycle    105,782.79
    Total L2 Elapsed Cycles          cycle   237,714,240
    Average SM Active Cycles         cycle     16,781.20
    Total SM Elapsed Cycles          cycle   292,909,708
    Average SMSP Active Cycles       cycle     16,742.86
    Total SMSP Elapsed Cycles        cycle 1,171,638,832
    -------------------------- ----------- -------------

  Pearson(double *, double *, double *) (1, 1, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         2.62
    SM Frequency            cycle/nsecond         1.53
    Elapsed Cycles                  cycle    2,219,080
    Memory Throughput                   %         0.07
    DRAM Throughput                     %         0.07
    Duration                      msecond         1.45
    L1/TEX Cache Throughput             %         4.52
    L2 Cache Throughput                 %         0.07
    SM Active Cycles                cycle    16,792.52
    Compute (SM) Throughput             %         0.21
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Threads                                   thread             512
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 99.24%                                                                                          
          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 132             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        24.93
    Achieved Active Warps Per SM           warp        15.95
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50.15%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (24.9%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      2,618.50
    Total DRAM Elapsed Cycles        cycle   182,329,344
    Average L1 Active Cycles         cycle     16,792.52
    Total L1 Elapsed Cycles          cycle   292,897,340
    Average L2 Active Cycles         cycle     98,180.09
    Total L2 Elapsed Cycles          cycle   237,756,768
    Average SM Active Cycles         cycle     16,792.52
    Total SM Elapsed Cycles          cycle   292,897,340
    Average SMSP Active Cycles       cycle     16,742.75
    Total SMSP Elapsed Cycles        cycle 1,171,589,360
    -------------------------- ----------- -------------

  Add(double *, double *, double) (128, 1, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         2.62
    SM Frequency            cycle/nsecond         1.52
    Elapsed Cycles                  cycle       20,769
    Memory Throughput                   %         7.35
    DRAM Throughput                     %         7.35
    Duration                      usecond        13.60
    L1/TEX Cache Throughput             %         6.75
    L2 Cache Throughput                 %        14.33
    SM Active Cycles                cycle    13,434.98
    Compute (SM) Throughput             %        14.12
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.2 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Threads                                   thread          65,536
    Uses Green Context                                             0
    Waves Per SM                                                0.24
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 3.03%                                                                                           
          The grid for this launch is configured to execute only 128 blocks, which is less than the GPU's 132           
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit SM                        block           32
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        24.78
    Achieved Active Warps Per SM           warp        15.86
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75.22%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     2,613.17
    Total DRAM Elapsed Cycles        cycle    1,707,264
    Average L1 Active Cycles         cycle    13,434.98
    Total L1 Elapsed Cycles          cycle    2,735,424
    Average L2 Active Cycles         cycle    12,422.19
    Total L2 Elapsed Cycles          cycle    2,211,360
    Average SM Active Cycles         cycle    13,434.98
    Total SM Elapsed Cycles          cycle    2,735,424
    Average SMSP Active Cycles       cycle    13,369.96
    Total SMSP Elapsed Cycles        cycle   10,941,696
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.656%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 10.27% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.418%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 9.95% above the average, while the minimum instance value is 100.00% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.656%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 10.27% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.108%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L2 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 9.47% above the average, while the minimum instance value is 8.51% below    
          the average.                                                                                                  

  SpatialFilter(double *, double *) (128, 1, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         2.62
    SM Frequency            cycle/nsecond         1.53
    Elapsed Cycles                  cycle      721,257
    Memory Throughput                   %         2.69
    DRAM Throughput                     %         0.11
    Duration                      usecond       471.42
    L1/TEX Cache Throughput             %         2.96
    L2 Cache Throughput                 %         1.30
    SM Active Cycles                cycle   656,710.43
    Compute (SM) Throughput             %        36.18
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 1.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              66
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Threads                                   thread          65,536
    Uses Green Context                                             0
    Waves Per SM                                                0.97
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 3.03%                                                                                           
          The grid for this launch is configured to execute only 128 blocks, which is less than the GPU's 132           
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit SM                        block           32
    Block Limit Registers                 block            1
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        24.58
    Achieved Active Warps Per SM           warp        15.73
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (25.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     1,318.50
    Total DRAM Elapsed Cycles        cycle   59,260,416
    Average L1 Active Cycles         cycle   656,710.43
    Total L1 Elapsed Cycles          cycle   95,188,176
    Average L2 Active Cycles         cycle   151,980.46
    Total L2 Elapsed Cycles          cycle   77,246,496
    Average SM Active Cycles         cycle   656,710.43
    Total SM Elapsed Cycles          cycle   95,188,176
    Average SMSP Active Cycles       cycle   650,851.80
    Total SMSP Elapsed Cycles        cycle  380,752,704
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.496%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 8.23% above the average, while the minimum instance value is 100.00% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.211%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 9.10% above the average, while the minimum instance value is 100.00% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.496%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 8.23% above the average, while the minimum instance value is 100.00% below the      
          average.                                                                                                      

  Pearson(double *, double *, double *) (1, 1, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         2.62
    SM Frequency            cycle/nsecond         1.53
    Elapsed Cycles                  cycle    2,217,751
    Memory Throughput                   %         0.07
    DRAM Throughput                     %         0.07
    Duration                      msecond         1.45
    L1/TEX Cache Throughput             %         4.53
    L2 Cache Throughput                 %         0.07
    SM Active Cycles                cycle    16,780.89
    Compute (SM) Throughput             %         0.21
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Threads                                   thread             512
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 99.24%                                                                                          
          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 132             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        24.93
    Achieved Active Warps Per SM           warp        15.95
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50.14%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (24.9%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      2,618.50
    Total DRAM Elapsed Cycles        cycle   182,220,288
    Average L1 Active Cycles         cycle     16,780.89
    Total L1 Elapsed Cycles          cycle   292,717,742
    Average L2 Active Cycles         cycle    111,821.20
    Total L2 Elapsed Cycles          cycle   237,666,816
    Average SM Active Cycles         cycle     16,780.89
    Total SM Elapsed Cycles          cycle   292,717,742
    Average SMSP Active Cycles       cycle     16,749.57
    Total SMSP Elapsed Cycles        cycle 1,170,870,968
    -------------------------- ----------- -------------

  Pearson(double *, double *, double *) (1, 1, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         2.62
    SM Frequency            cycle/nsecond         1.53
    Elapsed Cycles                  cycle    2,216,845
    Memory Throughput                   %         0.07
    DRAM Throughput                     %         0.07
    Duration                      msecond         1.45
    L1/TEX Cache Throughput             %         4.53
    L2 Cache Throughput                 %         0.08
    SM Active Cycles                cycle    16,770.61
    Compute (SM) Throughput             %         0.21
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Threads                                   thread             512
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 99.24%                                                                                          
          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 132             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        24.93
    Achieved Active Warps Per SM           warp        15.95
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50.15%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (24.9%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      2,618.17
    Total DRAM Elapsed Cycles        cycle   182,145,536
    Average L1 Active Cycles         cycle     16,770.61
    Total L1 Elapsed Cycles          cycle   292,565,998
    Average L2 Active Cycles         cycle     98,320.61
    Total L2 Elapsed Cycles          cycle   237,554,496
    Average SM Active Cycles         cycle     16,770.61
    Total SM Elapsed Cycles          cycle   292,565,998
    Average SMSP Active Cycles       cycle     16,725.98
    Total SMSP Elapsed Cycles        cycle 1,170,263,992
    -------------------------- ----------- -------------

  Add(double *, double *, double) (128, 1, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         2.61
    SM Frequency            cycle/nsecond         1.52
    Elapsed Cycles                  cycle       20,345
    Memory Throughput                   %         7.51
    DRAM Throughput                     %         7.51
    Duration                      usecond        13.31
    L1/TEX Cache Throughput             %         6.90
    L2 Cache Throughput                 %        14.61
    SM Active Cycles                cycle    13,137.18
    Compute (SM) Throughput             %        14.00
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.2 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Threads                                   thread          65,536
    Uses Green Context                                             0
    Waves Per SM                                                0.24
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 3.03%                                                                                           
          The grid for this launch is configured to execute only 128 blocks, which is less than the GPU's 132           
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit SM                        block           32
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        24.81
    Achieved Active Warps Per SM           warp        15.88
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75.19%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     2,612.83
    Total DRAM Elapsed Cycles        cycle    1,670,912
    Average L1 Active Cycles         cycle    13,137.18
    Total L1 Elapsed Cycles          cycle    2,679,812
    Average L2 Active Cycles         cycle    11,901.03
    Total L2 Elapsed Cycles          cycle    2,164,800
    Average SM Active Cycles         cycle    13,137.18
    Total SM Elapsed Cycles          cycle    2,679,812
    Average SMSP Active Cycles       cycle    12,997.31
    Total SMSP Elapsed Cycles        cycle   10,719,248
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.576%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 8.62% above the average, while the minimum instance value is 100.00% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.114%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 11.11% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.576%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 8.62% above the average, while the minimum instance value is 100.00% below the      
          average.                                                                                                      

  SpatialFilter(double *, double *) (128, 1, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         2.62
    SM Frequency            cycle/nsecond         1.53
    Elapsed Cycles                  cycle      721,061
    Memory Throughput                   %         2.70
    DRAM Throughput                     %         0.11
    Duration                      usecond       471.30
    L1/TEX Cache Throughput             %         2.94
    L2 Cache Throughput                 %         1.29
    SM Active Cycles                cycle   659,946.05
    Compute (SM) Throughput             %        36.43
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 1.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              66
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Threads                                   thread          65,536
    Uses Green Context                                             0
    Waves Per SM                                                0.97
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 3.03%                                                                                           
          The grid for this launch is configured to execute only 128 blocks, which is less than the GPU's 132           
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit SM                        block           32
    Block Limit Registers                 block            1
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        24.60
    Achieved Active Warps Per SM           warp        15.74
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (25.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     1,319.50
    Total DRAM Elapsed Cycles        cycle   59,246,592
    Average L1 Active Cycles         cycle   659,946.05
    Total L1 Elapsed Cycles          cycle   95,160,332
    Average L2 Active Cycles         cycle   129,870.69
    Total L2 Elapsed Cycles          cycle   77,201,184
    Average SM Active Cycles         cycle   659,946.05
    Total SM Elapsed Cycles          cycle   95,160,332
    Average SMSP Active Cycles       cycle   654,122.22
    Total SMSP Elapsed Cycles        cycle  380,641,328
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.128%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 7.79% above the average, while the minimum instance value is 100.00% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.77%                                                                                           
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 8.56% above the average, while the minimum instance value is 100.00% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.128%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 7.79% above the average, while the minimum instance value is 100.00% below the      
          average.                                                                                                      

  Pearson(double *, double *, double *) (1, 1, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         2.62
    SM Frequency            cycle/nsecond         1.53
    Elapsed Cycles                  cycle    2,219,421
    Memory Throughput                   %         0.07
    DRAM Throughput                     %         0.07
    Duration                      msecond         1.45
    L1/TEX Cache Throughput             %         4.53
    L2 Cache Throughput                 %         0.08
    SM Active Cycles                cycle    16,772.25
    Compute (SM) Throughput             %         0.21
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Threads                                   thread             512
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 99.24%                                                                                          
          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 132             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        24.93
    Achieved Active Warps Per SM           warp        15.95
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50.14%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (24.9%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      2,618.33
    Total DRAM Elapsed Cycles        cycle   182,359,296
    Average L1 Active Cycles         cycle     16,772.25
    Total L1 Elapsed Cycles          cycle   292,938,670
    Average L2 Active Cycles         cycle    101,929.73
    Total L2 Elapsed Cycles          cycle   237,820,224
    Average SM Active Cycles         cycle     16,772.25
    Total SM Elapsed Cycles          cycle   292,938,670
    Average SMSP Active Cycles       cycle     16,742.84
    Total SMSP Elapsed Cycles        cycle 1,171,754,680
    -------------------------- ----------- -------------

  Pearson(double *, double *, double *) (1, 1, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         2.62
    SM Frequency            cycle/nsecond         1.53
    Elapsed Cycles                  cycle    2,217,562
    Memory Throughput                   %         0.07
    DRAM Throughput                     %         0.07
    Duration                      msecond         1.45
    L1/TEX Cache Throughput             %         4.53
    L2 Cache Throughput                 %         0.08
    SM Active Cycles                cycle    16,762.75
    Compute (SM) Throughput             %         0.21
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Threads                                   thread             512
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 99.24%                                                                                          
          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 132             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        24.93
    Achieved Active Warps Per SM           warp        15.95
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50.15%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (24.9%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      2,618.33
    Total DRAM Elapsed Cycles        cycle   182,203,008
    Average L1 Active Cycles         cycle     16,762.75
    Total L1 Elapsed Cycles          cycle   292,695,694
    Average L2 Active Cycles         cycle     97,392.05
    Total L2 Elapsed Cycles          cycle   237,533,472
    Average SM Active Cycles         cycle     16,762.75
    Total SM Elapsed Cycles          cycle   292,695,694
    Average SMSP Active Cycles       cycle     16,742.07
    Total SMSP Elapsed Cycles        cycle 1,170,782,776
    -------------------------- ----------- -------------

  Add(double *, double *, double) (128, 1, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         2.62
    SM Frequency            cycle/nsecond         1.52
    Elapsed Cycles                  cycle       21,357
    Memory Throughput                   %         7.14
    DRAM Throughput                     %         7.14
    Duration                      usecond        13.98
    L1/TEX Cache Throughput             %         6.72
    L2 Cache Throughput                 %        13.98
    SM Active Cycles                cycle    13,488.48
    Compute (SM) Throughput             %        13.77
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.2 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Threads                                   thread          65,536
    Uses Green Context                                             0
    Waves Per SM                                                0.24
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 3.03%                                                                                           
          The grid for this launch is configured to execute only 128 blocks, which is less than the GPU's 132           
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit SM                        block           32
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        24.81
    Achieved Active Warps Per SM           warp        15.88
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75.19%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     2,612.67
    Total DRAM Elapsed Cycles        cycle    1,755,648
    Average L1 Active Cycles         cycle    13,488.48
    Total L1 Elapsed Cycles          cycle    2,812,132
    Average L2 Active Cycles         cycle    12,162.54
    Total L2 Elapsed Cycles          cycle    2,274,816
    Average SM Active Cycles         cycle    13,488.48
    Total SM Elapsed Cycles          cycle    2,812,132
    Average SMSP Active Cycles       cycle    13,369.77
    Total SMSP Elapsed Cycles        cycle   11,248,528
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.77%                                                                                           
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 9.11% above the average, while the minimum instance value is 100.00% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.77%                                                                                           
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 9.11% above the average, while the minimum instance value is 100.00% below the      
          average.                                                                                                      

  SpatialFilter(double *, double *) (128, 1, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         2.62
    SM Frequency            cycle/nsecond         1.53
    Elapsed Cycles                  cycle      718,747
    Memory Throughput                   %         2.70
    DRAM Throughput                     %         0.11
    Duration                      usecond       469.82
    L1/TEX Cache Throughput             %         2.96
    L2 Cache Throughput                 %         1.30
    SM Active Cycles                cycle   654,793.20
    Compute (SM) Throughput             %        36.22
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 1.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              66
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Threads                                   thread          65,536
    Uses Green Context                                             0
    Waves Per SM                                                0.97
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 3.03%                                                                                           
          The grid for this launch is configured to execute only 128 blocks, which is less than the GPU's 132           
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit SM                        block           32
    Block Limit Registers                 block            1
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        24.59
    Achieved Active Warps Per SM           warp        15.74
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (25.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        1,318
    Total DRAM Elapsed Cycles        cycle   59,055,616
    Average L1 Active Cycles         cycle   654,793.20
    Total L1 Elapsed Cycles          cycle   94,855,244
    Average L2 Active Cycles         cycle   148,435.23
    Total L2 Elapsed Cycles          cycle   76,974,048
    Average SM Active Cycles         cycle   654,793.20
    Total SM Elapsed Cycles          cycle   94,855,244
    Average SMSP Active Cycles       cycle   649,355.54
    Total SMSP Elapsed Cycles        cycle  379,420,976
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.442%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 8.17% above the average, while the minimum instance value is 100.00% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.048%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 8.91% above the average, while the minimum instance value is 100.00% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.442%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 8.17% above the average, while the minimum instance value is 100.00% below the      
          average.                                                                                                      

  Pearson(double *, double *, double *) (1, 1, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         2.62
    SM Frequency            cycle/nsecond         1.53
    Elapsed Cycles                  cycle    2,219,233
    Memory Throughput                   %         0.07
    DRAM Throughput                     %         0.07
    Duration                      msecond         1.45
    L1/TEX Cache Throughput             %         4.53
    L2 Cache Throughput                 %         0.07
    SM Active Cycles                cycle    16,776.64
    Compute (SM) Throughput             %         0.21
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Threads                                   thread             512
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 99.24%                                                                                          
          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 132             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        24.93
    Achieved Active Warps Per SM           warp        15.95
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50.15%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (24.9%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle         2,619
    Total DRAM Elapsed Cycles        cycle   182,342,016
    Average L1 Active Cycles         cycle     16,776.64
    Total L1 Elapsed Cycles          cycle   292,891,634
    Average L2 Active Cycles         cycle    101,703.55
    Total L2 Elapsed Cycles          cycle   237,831,456
    Average SM Active Cycles         cycle     16,776.64
    Total SM Elapsed Cycles          cycle   292,891,634
    Average SMSP Active Cycles       cycle     16,759.71
    Total SMSP Elapsed Cycles        cycle 1,171,566,536
    -------------------------- ----------- -------------

  Pearson(double *, double *, double *) (1, 1, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         2.62
    SM Frequency            cycle/nsecond         1.53
    Elapsed Cycles                  cycle    2,218,949
    Memory Throughput                   %         0.07
    DRAM Throughput                     %         0.07
    Duration                      msecond         1.45
    L1/TEX Cache Throughput             %         4.53
    L2 Cache Throughput                 %         0.08
    SM Active Cycles                cycle    16,761.76
    Compute (SM) Throughput             %         0.21
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Threads                                   thread             512
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 99.24%                                                                                          
          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 132             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        24.93
    Achieved Active Warps Per SM           warp        15.95
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50.14%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (24.9%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      2,618.17
    Total DRAM Elapsed Cycles        cycle   182,317,568
    Average L1 Active Cycles         cycle     16,761.76
    Total L1 Elapsed Cycles          cycle   292,870,718
    Average L2 Active Cycles         cycle     99,706.04
    Total L2 Elapsed Cycles          cycle   237,866,976
    Average SM Active Cycles         cycle     16,761.76
    Total SM Elapsed Cycles          cycle   292,870,718
    Average SMSP Active Cycles       cycle     16,748.18
    Total SMSP Elapsed Cycles        cycle 1,171,482,872
    -------------------------- ----------- -------------

  Add(double *, double *, double) (128, 1, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         2.61
    SM Frequency            cycle/nsecond         1.52
    Elapsed Cycles                  cycle       20,856
    Memory Throughput                   %         7.32
    DRAM Throughput                     %         7.32
    Duration                      usecond        13.66
    L1/TEX Cache Throughput             %         6.92
    L2 Cache Throughput                 %        14.17
    SM Active Cycles                cycle    13,099.67
    Compute (SM) Throughput             %        13.65
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.2 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Threads                                   thread          65,536
    Uses Green Context                                             0
    Waves Per SM                                                0.24
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 3.03%                                                                                           
          The grid for this launch is configured to execute only 128 blocks, which is less than the GPU's 132           
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit SM                        block           32
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        24.79
    Achieved Active Warps Per SM           warp        15.87
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75.21%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     2,612.83
    Total DRAM Elapsed Cycles        cycle    1,713,408
    Average L1 Active Cycles         cycle    13,099.67
    Total L1 Elapsed Cycles          cycle    2,747,952
    Average L2 Active Cycles         cycle    11,997.54
    Total L2 Elapsed Cycles          cycle    2,220,960
    Average SM Active Cycles         cycle    13,099.67
    Total SM Elapsed Cycles          cycle    2,747,952
    Average SMSP Active Cycles       cycle    13,038.06
    Total SMSP Elapsed Cycles        cycle   10,991,808
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.651%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 10.57% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.527%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 8.82% above the average, while the minimum instance value is 100.00% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.651%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 10.57% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  SpatialFilter(double *, double *) (128, 1, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         2.62
    SM Frequency            cycle/nsecond         1.53
    Elapsed Cycles                  cycle      721,409
    Memory Throughput                   %         2.69
    DRAM Throughput                     %         0.11
    Duration                      usecond       471.55
    L1/TEX Cache Throughput             %         2.94
    L2 Cache Throughput                 %         1.29
    SM Active Cycles                cycle   659,927.94
    Compute (SM) Throughput             %        36.42
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 1.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              66
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Threads                                   thread          65,536
    Uses Green Context                                             0
    Waves Per SM                                                0.97
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 3.03%                                                                                           
          The grid for this launch is configured to execute only 128 blocks, which is less than the GPU's 132           
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit SM                        block           32
    Block Limit Registers                 block            1
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        24.60
    Achieved Active Warps Per SM           warp        15.74
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (25.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     1,319.33
    Total DRAM Elapsed Cycles        cycle   59,276,800
    Average L1 Active Cycles         cycle   659,927.94
    Total L1 Elapsed Cycles          cycle   95,202,560
    Average L2 Active Cycles         cycle   132,259.20
    Total L2 Elapsed Cycles          cycle   77,266,848
    Average SM Active Cycles         cycle   659,927.94
    Total SM Elapsed Cycles          cycle   95,202,560
    Average SMSP Active Cycles       cycle   654,297.95
    Total SMSP Elapsed Cycles        cycle  380,810,240
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.106%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 7.77% above the average, while the minimum instance value is 100.00% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.836%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 8.64% above the average, while the minimum instance value is 100.00% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.106%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 7.77% above the average, while the minimum instance value is 100.00% below the      
          average.                                                                                                      

  Pearson(double *, double *, double *) (1, 1, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         2.62
    SM Frequency            cycle/nsecond         1.53
    Elapsed Cycles                  cycle    2,220,330
    Memory Throughput                   %         0.07
    DRAM Throughput                     %         0.07
    Duration                      msecond         1.45
    L1/TEX Cache Throughput             %         4.53
    L2 Cache Throughput                 %         0.08
    SM Active Cycles                cycle    16,786.91
    Compute (SM) Throughput             %         0.21
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Threads                                   thread             512
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 99.24%                                                                                          
          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 132             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        24.93
    Achieved Active Warps Per SM           warp        15.95
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50.15%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (24.9%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      2,618.67
    Total DRAM Elapsed Cycles        cycle   182,433,024
    Average L1 Active Cycles         cycle     16,786.91
    Total L1 Elapsed Cycles          cycle   293,045,456
    Average L2 Active Cycles         cycle    100,157.71
    Total L2 Elapsed Cycles          cycle   237,856,128
    Average SM Active Cycles         cycle     16,786.91
    Total SM Elapsed Cycles          cycle   293,045,456
    Average SMSP Active Cycles       cycle     16,739.47
    Total SMSP Elapsed Cycles        cycle 1,172,181,824
    -------------------------- ----------- -------------

  Pearson(double *, double *, double *) (1, 1, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         2.62
    SM Frequency            cycle/nsecond         1.53
    Elapsed Cycles                  cycle    2,219,092
    Memory Throughput                   %         0.07
    DRAM Throughput                     %         0.07
    Duration                      msecond         1.45
    L1/TEX Cache Throughput             %         4.54
    L2 Cache Throughput                 %         0.07
    SM Active Cycles                cycle    16,754.62
    Compute (SM) Throughput             %         0.21
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Threads                                   thread             512
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 99.24%                                                                                          
          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 132             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        24.93
    Achieved Active Warps Per SM           warp        15.95
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50.15%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (24.9%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      2,619.33
    Total DRAM Elapsed Cycles        cycle   182,330,880
    Average L1 Active Cycles         cycle     16,754.62
    Total L1 Elapsed Cycles          cycle   292,873,910
    Average L2 Active Cycles         cycle    102,217.07
    Total L2 Elapsed Cycles          cycle   237,724,512
    Average SM Active Cycles         cycle     16,754.62
    Total SM Elapsed Cycles          cycle   292,873,910
    Average SMSP Active Cycles       cycle     16,732.91
    Total SMSP Elapsed Cycles        cycle 1,171,495,640
    -------------------------- ----------- -------------

  Add(double *, double *, double) (128, 1, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         2.61
    SM Frequency            cycle/nsecond         1.52
    Elapsed Cycles                  cycle       20,682
    Memory Throughput                   %         7.39
    DRAM Throughput                     %         7.39
    Duration                      usecond        13.57
    L1/TEX Cache Throughput             %         6.73
    L2 Cache Throughput                 %        14.34
    SM Active Cycles                cycle    13,479.27
    Compute (SM) Throughput             %        14.27
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.2 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Threads                                   thread          65,536
    Uses Green Context                                             0
    Waves Per SM                                                0.24
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 3.03%                                                                                           
          The grid for this launch is configured to execute only 128 blocks, which is less than the GPU's 132           
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit SM                        block           32
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        24.82
    Achieved Active Warps Per SM           warp        15.88
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75.18%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     2,613.17
    Total DRAM Elapsed Cycles        cycle    1,697,536
    Average L1 Active Cycles         cycle    13,479.27
    Total L1 Elapsed Cycles          cycle    2,724,546
    Average L2 Active Cycles         cycle    12,288.02
    Total L2 Elapsed Cycles          cycle    2,200,416
    Average SM Active Cycles         cycle    13,479.27
    Total SM Elapsed Cycles          cycle    2,724,546
    Average SMSP Active Cycles       cycle    13,390.34
    Total SMSP Elapsed Cycles        cycle   10,898,184
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.33%                                                                                           
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 9.69% above the average, while the minimum instance value is 100.00% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.235%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 11.15% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.33%                                                                                           
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 9.69% above the average, while the minimum instance value is 100.00% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.282%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L2 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 9.85% above the average, while the minimum instance value is 10.49% below   
          the average.                                                                                                  

  SpatialFilter(double *, double *) (128, 1, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         2.62
    SM Frequency            cycle/nsecond         1.53
    Elapsed Cycles                  cycle      714,192
    Memory Throughput                   %         2.72
    DRAM Throughput                     %         0.11
    Duration                      usecond       466.82
    L1/TEX Cache Throughput             %         2.97
    L2 Cache Throughput                 %         1.32
    SM Active Cycles                cycle   652,622.33
    Compute (SM) Throughput             %        36.34
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 1.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              66
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Threads                                   thread          65,536
    Uses Green Context                                             0
    Waves Per SM                                                0.97
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 3.03%                                                                                           
          The grid for this launch is configured to execute only 128 blocks, which is less than the GPU's 132           
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit SM                        block           32
    Block Limit Registers                 block            1
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        24.59
    Achieved Active Warps Per SM           warp        15.74
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (25.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     1,317.50
    Total DRAM Elapsed Cycles        cycle   58,681,344
    Average L1 Active Cycles         cycle   652,622.33
    Total L1 Elapsed Cycles          cycle   94,245,126
    Average L2 Active Cycles         cycle   143,281.42
    Total L2 Elapsed Cycles          cycle   76,483,872
    Average SM Active Cycles         cycle   652,622.33
    Total SM Elapsed Cycles          cycle   94,245,126
    Average SMSP Active Cycles       cycle   647,018.95
    Total SMSP Elapsed Cycles        cycle  376,980,504
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.217%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 7.90% above the average, while the minimum instance value is 100.00% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.852%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 8.66% above the average, while the minimum instance value is 100.00% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.217%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 7.90% above the average, while the minimum instance value is 100.00% below the      
          average.                                                                                                      

  Pearson(double *, double *, double *) (1, 1, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         2.62
    SM Frequency            cycle/nsecond         1.53
    Elapsed Cycles                  cycle    2,222,502
    Memory Throughput                   %         0.07
    DRAM Throughput                     %         0.07
    Duration                      msecond         1.45
    L1/TEX Cache Throughput             %         4.53
    L2 Cache Throughput                 %         0.07
    SM Active Cycles                cycle    16,780.89
    Compute (SM) Throughput             %         0.21
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Threads                                   thread             512
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 99.24%                                                                                          
          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 132             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        24.93
    Achieved Active Warps Per SM           warp        15.95
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50.14%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (24.9%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      2,789.17
    Total DRAM Elapsed Cycles        cycle   182,611,968
    Average L1 Active Cycles         cycle     16,780.89
    Total L1 Elapsed Cycles          cycle   293,334,228
    Average L2 Active Cycles         cycle    100,648.21
    Total L2 Elapsed Cycles          cycle   238,091,904
    Average SM Active Cycles         cycle     16,780.89
    Total SM Elapsed Cycles          cycle   293,334,228
    Average SMSP Active Cycles       cycle     16,744.83
    Total SMSP Elapsed Cycles        cycle 1,173,336,912
    -------------------------- ----------- -------------

  Pearson(double *, double *, double *) (1, 1, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         2.62
    SM Frequency            cycle/nsecond         1.53
    Elapsed Cycles                  cycle    2,216,840
    Memory Throughput                   %         0.07
    DRAM Throughput                     %         0.07
    Duration                      msecond         1.45
    L1/TEX Cache Throughput             %         4.53
    L2 Cache Throughput                 %         0.07
    SM Active Cycles                cycle    16,764.67
    Compute (SM) Throughput             %         0.21
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Threads                                   thread             512
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 99.24%                                                                                          
          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 132             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        24.93
    Achieved Active Warps Per SM           warp        15.95
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50.15%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (24.9%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      2,618.33
    Total DRAM Elapsed Cycles        cycle   182,145,024
    Average L1 Active Cycles         cycle     16,764.67
    Total L1 Elapsed Cycles          cycle   292,581,346
    Average L2 Active Cycles         cycle     97,860.20
    Total L2 Elapsed Cycles          cycle   237,585,600
    Average SM Active Cycles         cycle     16,764.67
    Total SM Elapsed Cycles          cycle   292,581,346
    Average SMSP Active Cycles       cycle     16,742.68
    Total SMSP Elapsed Cycles        cycle 1,170,325,384
    -------------------------- ----------- -------------

  Add(double *, double *, double) (128, 1, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         2.61
    SM Frequency            cycle/nsecond         1.52
    Elapsed Cycles                  cycle       21,194
    Memory Throughput                   %         7.21
    DRAM Throughput                     %         7.21
    Duration                      usecond        13.89
    L1/TEX Cache Throughput             %         6.59
    L2 Cache Throughput                 %        14.08
    SM Active Cycles                cycle    13,766.89
    Compute (SM) Throughput             %        13.93
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.2 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Threads                                   thread          65,536
    Uses Green Context                                             0
    Waves Per SM                                                0.24
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 3.03%                                                                                           
          The grid for this launch is configured to execute only 128 blocks, which is less than the GPU's 132           
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit SM                        block           32
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        24.83
    Achieved Active Warps Per SM           warp        15.89
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75.17%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        2,613
    Total DRAM Elapsed Cycles        cycle    1,740,672
    Average L1 Active Cycles         cycle    13,766.89
    Total L1 Elapsed Cycles          cycle    2,791,120
    Average L2 Active Cycles         cycle    12,217.15
    Total L2 Elapsed Cycles          cycle    2,257,632
    Average SM Active Cycles         cycle    13,766.89
    Total SM Elapsed Cycles          cycle    2,791,120
    Average SMSP Active Cycles       cycle    13,431.86
    Total SMSP Elapsed Cycles        cycle   11,164,480
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.424%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 9.87% above the average, while the minimum instance value is 100.00% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.845%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 9.20% above the average, while the minimum instance value is 100.00% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.424%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 9.87% above the average, while the minimum instance value is 100.00% below the      
          average.                                                                                                      

  SpatialFilter(double *, double *) (128, 1, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         2.62
    SM Frequency            cycle/nsecond         1.53
    Elapsed Cycles                  cycle      716,145
    Memory Throughput                   %         2.71
    DRAM Throughput                     %         0.11
    Duration                      usecond       468.10
    L1/TEX Cache Throughput             %         2.97
    L2 Cache Throughput                 %         1.31
    SM Active Cycles                cycle   653,232.39
    Compute (SM) Throughput             %        36.25
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 1.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              66
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Threads                                   thread          65,536
    Uses Green Context                                             0
    Waves Per SM                                                0.97
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 3.03%                                                                                           
          The grid for this launch is configured to execute only 128 blocks, which is less than the GPU's 132           
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit SM                        block           32
    Block Limit Registers                 block            1
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        24.57
    Achieved Active Warps Per SM           warp        15.73
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (25.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     1,317.83
    Total DRAM Elapsed Cycles        cycle   58,841,856
    Average L1 Active Cycles         cycle   653,232.39
    Total L1 Elapsed Cycles          cycle   94,505,376
    Average L2 Active Cycles         cycle   144,281.38
    Total L2 Elapsed Cycles          cycle   76,713,120
    Average SM Active Cycles         cycle   653,232.39
    Total SM Elapsed Cycles          cycle   94,505,376
    Average SMSP Active Cycles       cycle   647,377.45
    Total SMSP Elapsed Cycles        cycle  378,021,504
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.428%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 8.14% above the average, while the minimum instance value is 100.00% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.165%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 9.03% above the average, while the minimum instance value is 100.00% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.428%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 8.14% above the average, while the minimum instance value is 100.00% below the      
          average.                                                                                                      

  Pearson(double *, double *, double *) (1, 1, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         2.62
    SM Frequency            cycle/nsecond         1.53
    Elapsed Cycles                  cycle    2,219,867
    Memory Throughput                   %         0.07
    DRAM Throughput                     %         0.07
    Duration                      msecond         1.45
    L1/TEX Cache Throughput             %         4.52
    L2 Cache Throughput                 %         0.07
    SM Active Cycles                cycle    16,789.23
    Compute (SM) Throughput             %         0.21
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Threads                                   thread             512
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 99.24%                                                                                          
          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 132             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        24.93
    Achieved Active Warps Per SM           warp        15.95
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50.14%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (24.9%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      2,618.33
    Total DRAM Elapsed Cycles        cycle   182,392,832
    Average L1 Active Cycles         cycle     16,789.23
    Total L1 Elapsed Cycles          cycle   292,989,494
    Average L2 Active Cycles         cycle    102,448.50
    Total L2 Elapsed Cycles          cycle   237,800,544
    Average SM Active Cycles         cycle     16,789.23
    Total SM Elapsed Cycles          cycle   292,989,494
    Average SMSP Active Cycles       cycle     16,745.60
    Total SMSP Elapsed Cycles        cycle 1,171,957,976
    -------------------------- ----------- -------------

  Pearson(double *, double *, double *) (1, 1, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         2.62
    SM Frequency            cycle/nsecond         1.53
    Elapsed Cycles                  cycle    2,217,922
    Memory Throughput                   %         0.07
    DRAM Throughput                     %         0.07
    Duration                      msecond         1.45
    L1/TEX Cache Throughput             %         4.53
    L2 Cache Throughput                 %         0.07
    SM Active Cycles                cycle    16,769.89
    Compute (SM) Throughput             %         0.21
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Threads                                   thread             512
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 99.24%                                                                                          
          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 132             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        24.93
    Achieved Active Warps Per SM           warp        15.95
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50.15%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (24.9%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      2,618.67
    Total DRAM Elapsed Cycles        cycle   182,235,648
    Average L1 Active Cycles         cycle     16,769.89
    Total L1 Elapsed Cycles          cycle   292,704,864
    Average L2 Active Cycles         cycle     97,824.56
    Total L2 Elapsed Cycles          cycle   237,595,104
    Average SM Active Cycles         cycle     16,769.89
    Total SM Elapsed Cycles          cycle   292,704,864
    Average SMSP Active Cycles       cycle     16,728.41
    Total SMSP Elapsed Cycles        cycle 1,170,819,456
    -------------------------- ----------- -------------

  Add(double *, double *, double) (128, 1, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         2.61
    SM Frequency            cycle/nsecond         1.52
    Elapsed Cycles                  cycle       20,884
    Memory Throughput                   %         7.31
    DRAM Throughput                     %         7.31
    Duration                      usecond        13.66
    L1/TEX Cache Throughput             %         6.73
    L2 Cache Throughput                 %        14.13
    SM Active Cycles                cycle    13,475.13
    Compute (SM) Throughput             %        14.10
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.2 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Threads                                   thread          65,536
    Uses Green Context                                             0
    Waves Per SM                                                0.24
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 3.03%                                                                                           
          The grid for this launch is configured to execute only 128 blocks, which is less than the GPU's 132           
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit SM                        block           32
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        24.79
    Achieved Active Warps Per SM           warp        15.86
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75.21%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     2,612.67
    Total DRAM Elapsed Cycles        cycle    1,714,688
    Average L1 Active Cycles         cycle    13,475.13
    Total L1 Elapsed Cycles          cycle    2,748,364
    Average L2 Active Cycles         cycle    11,949.06
    Total L2 Elapsed Cycles          cycle    2,222,976
    Average SM Active Cycles         cycle    13,475.13
    Total SM Elapsed Cycles          cycle    2,748,364
    Average SMSP Active Cycles       cycle    13,436.65
    Total SMSP Elapsed Cycles        cycle   10,993,456
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.159%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 11.06% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.98%                                                                                           
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 9.27% above the average, while the minimum instance value is 100.00% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.159%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 11.06% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  SpatialFilter(double *, double *) (128, 1, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         2.62
    SM Frequency            cycle/nsecond         1.53
    Elapsed Cycles                  cycle      717,226
    Memory Throughput                   %         2.71
    DRAM Throughput                     %         0.11
    Duration                      usecond       468.80
    L1/TEX Cache Throughput             %         2.97
    L2 Cache Throughput                 %         1.31
    SM Active Cycles                cycle   654,642.50
    Compute (SM) Throughput             %        36.28
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 1.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              66
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Threads                                   thread          65,536
    Uses Green Context                                             0
    Waves Per SM                                                0.97
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 3.03%                                                                                           
          The grid for this launch is configured to execute only 128 blocks, which is less than the GPU's 132           
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit SM                        block           32
    Block Limit Registers                 block            1
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        24.59
    Achieved Active Warps Per SM           warp        15.73
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (25.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     1,317.17
    Total DRAM Elapsed Cycles        cycle   58,929,536
    Average L1 Active Cycles         cycle   654,642.50
    Total L1 Elapsed Cycles          cycle   94,651,942
    Average L2 Active Cycles         cycle   150,305.99
    Total L2 Elapsed Cycles          cycle   76,802,784
    Average SM Active Cycles         cycle   654,642.50
    Total SM Elapsed Cycles          cycle   94,651,942
    Average SMSP Active Cycles       cycle   648,997.71
    Total SMSP Elapsed Cycles        cycle  378,607,768
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.311%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 8.01% above the average, while the minimum instance value is 100.00% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.951%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 8.79% above the average, while the minimum instance value is 100.00% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.311%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 8.01% above the average, while the minimum instance value is 100.00% below the      
          average.                                                                                                      

  Pearson(double *, double *, double *) (1, 1, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         2.62
    SM Frequency            cycle/nsecond         1.53
    Elapsed Cycles                  cycle    2,218,323
    Memory Throughput                   %         0.07
    DRAM Throughput                     %         0.07
    Duration                      msecond         1.45
    L1/TEX Cache Throughput             %         4.53
    L2 Cache Throughput                 %         0.07
    SM Active Cycles                cycle    16,788.81
    Compute (SM) Throughput             %         0.21
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Threads                                   thread             512
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 99.24%                                                                                          
          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 132             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        24.93
    Achieved Active Warps Per SM           warp        15.95
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50.15%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (24.9%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      2,622.33
    Total DRAM Elapsed Cycles        cycle   182,267,648
    Average L1 Active Cycles         cycle     16,788.81
    Total L1 Elapsed Cycles          cycle   292,785,156
    Average L2 Active Cycles         cycle    105,363.44
    Total L2 Elapsed Cycles          cycle   237,646,176
    Average SM Active Cycles         cycle     16,788.81
    Total SM Elapsed Cycles          cycle   292,785,156
    Average SMSP Active Cycles       cycle     16,765.60
    Total SMSP Elapsed Cycles        cycle 1,171,140,624
    -------------------------- ----------- -------------

  Pearson(double *, double *, double *) (1, 1, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         2.62
    SM Frequency            cycle/nsecond         1.53
    Elapsed Cycles                  cycle    2,220,501
    Memory Throughput                   %         0.07
    DRAM Throughput                     %         0.07
    Duration                      msecond         1.45
    L1/TEX Cache Throughput             %         4.53
    L2 Cache Throughput                 %         0.09
    SM Active Cycles                cycle    16,771.77
    Compute (SM) Throughput             %         0.21
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Threads                                   thread             512
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 99.24%                                                                                          
          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 132             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        24.93
    Achieved Active Warps Per SM           warp        15.95
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50.15%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (24.9%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      2,618.50
    Total DRAM Elapsed Cycles        cycle   182,446,080
    Average L1 Active Cycles         cycle     16,771.77
    Total L1 Elapsed Cycles          cycle   293,048,332
    Average L2 Active Cycles         cycle     98,198.12
    Total L2 Elapsed Cycles          cycle   237,947,136
    Average SM Active Cycles         cycle     16,771.77
    Total SM Elapsed Cycles          cycle   293,048,332
    Average SMSP Active Cycles       cycle     16,735.84
    Total SMSP Elapsed Cycles        cycle 1,172,193,328
    -------------------------- ----------- -------------

  Add(double *, double *, double) (128, 1, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         2.62
    SM Frequency            cycle/nsecond         1.53
    Elapsed Cycles                  cycle       20,942
    Memory Throughput                   %         7.29
    DRAM Throughput                     %         7.29
    Duration                      usecond        13.70
    L1/TEX Cache Throughput             %         6.81
    L2 Cache Throughput                 %        14.17
    SM Active Cycles                cycle    13,320.24
    Compute (SM) Throughput             %        13.60
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.2 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Threads                                   thread          65,536
    Uses Green Context                                             0
    Waves Per SM                                                0.24
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 3.03%                                                                                           
          The grid for this launch is configured to execute only 128 blocks, which is less than the GPU's 132           
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit SM                        block           32
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        24.81
    Achieved Active Warps Per SM           warp        15.88
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75.19%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     2,613.33
    Total DRAM Elapsed Cycles        cycle    1,720,320
    Average L1 Active Cycles         cycle    13,320.24
    Total L1 Elapsed Cycles          cycle    2,758,872
    Average L2 Active Cycles         cycle    11,968.91
    Total L2 Elapsed Cycles          cycle    2,229,696
    Average SM Active Cycles         cycle    13,320.24
    Total SM Elapsed Cycles          cycle    2,758,872
    Average SMSP Active Cycles       cycle    13,021.49
    Total SMSP Elapsed Cycles        cycle   11,035,488
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.474%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 11.73% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.653%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 9.07% above the average, while the minimum instance value is 100.00% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.474%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 11.73% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  SpatialFilter(double *, double *) (128, 1, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         2.62
    SM Frequency            cycle/nsecond         1.53
    Elapsed Cycles                  cycle      721,503
    Memory Throughput                   %         2.69
    DRAM Throughput                     %         0.11
    Duration                      usecond       471.62
    L1/TEX Cache Throughput             %         2.94
    L2 Cache Throughput                 %         1.29
    SM Active Cycles                cycle   659,975.62
    Compute (SM) Throughput             %        36.41
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 1.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              66
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Threads                                   thread          65,536
    Uses Green Context                                             0
    Waves Per SM                                                0.97
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 3.03%                                                                                           
          The grid for this launch is configured to execute only 128 blocks, which is less than the GPU's 132           
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit SM                        block           32
    Block Limit Registers                 block            1
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        24.59
    Achieved Active Warps Per SM           warp        15.74
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (25.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     1,319.67
    Total DRAM Elapsed Cycles        cycle   59,283,456
    Average L1 Active Cycles         cycle   659,975.62
    Total L1 Elapsed Cycles          cycle   95,217,610
    Average L2 Active Cycles         cycle   128,925.14
    Total L2 Elapsed Cycles          cycle   77,298,240
    Average SM Active Cycles         cycle   659,975.62
    Total SM Elapsed Cycles          cycle   95,217,610
    Average SMSP Active Cycles       cycle   654,079.99
    Total SMSP Elapsed Cycles        cycle  380,870,440
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.065%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 7.72% above the average, while the minimum instance value is 100.00% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.768%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 8.57% above the average, while the minimum instance value is 100.00% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.065%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 7.72% above the average, while the minimum instance value is 100.00% below the      
          average.                                                                                                      

  Pearson(double *, double *, double *) (1, 1, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         2.62
    SM Frequency            cycle/nsecond         1.53
    Elapsed Cycles                  cycle    2,219,981
    Memory Throughput                   %         0.07
    DRAM Throughput                     %         0.07
    Duration                      msecond         1.45
    L1/TEX Cache Throughput             %         4.53
    L2 Cache Throughput                 %         0.09
    SM Active Cycles                cycle    16,774.98
    Compute (SM) Throughput             %         0.21
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Threads                                   thread             512
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 99.24%                                                                                          
          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 132             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        24.93
    Achieved Active Warps Per SM           warp        15.95
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50.15%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (24.9%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      2,618.33
    Total DRAM Elapsed Cycles        cycle   182,403,072
    Average L1 Active Cycles         cycle     16,774.98
    Total L1 Elapsed Cycles          cycle   293,018,632
    Average L2 Active Cycles         cycle     98,383.32
    Total L2 Elapsed Cycles          cycle   237,910,464
    Average SM Active Cycles         cycle     16,774.98
    Total SM Elapsed Cycles          cycle   293,018,632
    Average SMSP Active Cycles       cycle     16,744.49
    Total SMSP Elapsed Cycles        cycle 1,172,074,528
    -------------------------- ----------- -------------

  Pearson(double *, double *, double *) (1, 1, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         2.62
    SM Frequency            cycle/nsecond         1.53
    Elapsed Cycles                  cycle    2,218,448
    Memory Throughput                   %         0.07
    DRAM Throughput                     %         0.07
    Duration                      msecond         1.45
    L1/TEX Cache Throughput             %         4.52
    L2 Cache Throughput                 %         0.09
    SM Active Cycles                cycle    16,786.94
    Compute (SM) Throughput             %         0.21
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Threads                                   thread             512
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 99.24%                                                                                          
          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 132             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        24.93
    Achieved Active Warps Per SM           warp        15.95
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50.15%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (24.9%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      2,791.50
    Total DRAM Elapsed Cycles        cycle   182,276,096
    Average L1 Active Cycles         cycle     16,786.94
    Total L1 Elapsed Cycles          cycle   292,804,768
    Average L2 Active Cycles         cycle     96,600.75
    Total L2 Elapsed Cycles          cycle   237,779,808
    Average SM Active Cycles         cycle     16,786.94
    Total SM Elapsed Cycles          cycle   292,804,768
    Average SMSP Active Cycles       cycle     16,739.57
    Total SMSP Elapsed Cycles        cycle 1,171,219,072
    -------------------------- ----------- -------------

  Add(double *, double *, double) (128, 1, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         2.61
    SM Frequency            cycle/nsecond         1.52
    Elapsed Cycles                  cycle       20,931
    Memory Throughput                   %         7.29
    DRAM Throughput                     %         7.29
    Duration                      usecond        13.73
    L1/TEX Cache Throughput             %         6.74
    L2 Cache Throughput                 %        14.26
    SM Active Cycles                cycle    13,456.59
    Compute (SM) Throughput             %        14.04
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.2 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Threads                                   thread          65,536
    Uses Green Context                                             0
    Waves Per SM                                                0.24
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 3.03%                                                                                           
          The grid for this launch is configured to execute only 128 blocks, which is less than the GPU's 132           
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit SM                        block           32
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        24.76
    Achieved Active Warps Per SM           warp        15.84
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75.24%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     2,612.67
    Total DRAM Elapsed Cycles        cycle    1,719,424
    Average L1 Active Cycles         cycle    13,456.59
    Total L1 Elapsed Cycles          cycle    2,757,678
    Average L2 Active Cycles         cycle    12,555.29
    Total L2 Elapsed Cycles          cycle    2,230,368
    Average SM Active Cycles         cycle    13,456.59
    Total SM Elapsed Cycles          cycle    2,757,678
    Average SMSP Active Cycles       cycle    13,483.03
    Total SMSP Elapsed Cycles        cycle   11,030,712
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.801%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 12.11% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.912%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 12.26% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.801%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 12.11% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  SpatialFilter(double *, double *) (128, 1, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         2.62
    SM Frequency            cycle/nsecond         1.53
    Elapsed Cycles                  cycle      720,063
    Memory Throughput                   %         2.70
    DRAM Throughput                     %         0.11
    Duration                      usecond       470.66
    L1/TEX Cache Throughput             %         2.96
    L2 Cache Throughput                 %         1.30
    SM Active Cycles                cycle   655,243.02
    Compute (SM) Throughput             %        36.17
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 1.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              66
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Threads                                   thread          65,536
    Uses Green Context                                             0
    Waves Per SM                                                0.97
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 3.03%                                                                                           
          The grid for this launch is configured to execute only 128 blocks, which is less than the GPU's 132           
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit SM                        block           32
    Block Limit Registers                 block            1
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        24.59
    Achieved Active Warps Per SM           warp        15.74
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (25.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        1,319
    Total DRAM Elapsed Cycles        cycle   59,164,416
    Average L1 Active Cycles         cycle   655,243.02
    Total L1 Elapsed Cycles          cycle   95,027,394
    Average L2 Active Cycles         cycle   151,218.69
    Total L2 Elapsed Cycles          cycle   77,107,776
    Average SM Active Cycles         cycle   655,243.02
    Total SM Elapsed Cycles          cycle   95,027,394
    Average SMSP Active Cycles       cycle   649,704.59
    Total SMSP Elapsed Cycles        cycle  380,109,576
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.595%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 8.34% above the average, while the minimum instance value is 100.00% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.175%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 9.06% above the average, while the minimum instance value is 100.00% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.595%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 8.34% above the average, while the minimum instance value is 100.00% below the      
          average.                                                                                                      

  Pearson(double *, double *, double *) (1, 1, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         2.62
    SM Frequency            cycle/nsecond         1.53
    Elapsed Cycles                  cycle    2,219,188
    Memory Throughput                   %         0.07
    DRAM Throughput                     %         0.07
    Duration                      msecond         1.45
    L1/TEX Cache Throughput             %         4.53
    L2 Cache Throughput                 %         0.06
    SM Active Cycles                cycle    16,774.83
    Compute (SM) Throughput             %         0.21
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Threads                                   thread             512
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 99.24%                                                                                          
          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 132             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        24.93
    Achieved Active Warps Per SM           warp        15.95
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50.15%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (24.9%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      2,618.50
    Total DRAM Elapsed Cycles        cycle   182,339,072
    Average L1 Active Cycles         cycle     16,774.83
    Total L1 Elapsed Cycles          cycle   292,909,462
    Average L2 Active Cycles         cycle    101,937.86
    Total L2 Elapsed Cycles          cycle   237,852,576
    Average SM Active Cycles         cycle     16,774.83
    Total SM Elapsed Cycles          cycle   292,909,462
    Average SMSP Active Cycles       cycle     16,753.37
    Total SMSP Elapsed Cycles        cycle 1,171,637,848
    -------------------------- ----------- -------------

  Pearson(double *, double *, double *) (1, 1, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         2.62
    SM Frequency            cycle/nsecond         1.53
    Elapsed Cycles                  cycle    2,218,301
    Memory Throughput                   %         0.07
    DRAM Throughput                     %         0.07
    Duration                      msecond         1.45
    L1/TEX Cache Throughput             %         4.53
    L2 Cache Throughput                 %         0.08
    SM Active Cycles                cycle    16,779.28
    Compute (SM) Throughput             %         0.21
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Threads                                   thread             512
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 99.24%                                                                                          
          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 132             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        24.93
    Achieved Active Warps Per SM           warp        15.95
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50.15%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (24.9%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      2,618.67
    Total DRAM Elapsed Cycles        cycle   182,266,368
    Average L1 Active Cycles         cycle     16,779.28
    Total L1 Elapsed Cycles          cycle   292,795,864
    Average L2 Active Cycles         cycle     96,903.27
    Total L2 Elapsed Cycles          cycle   237,742,464
    Average SM Active Cycles         cycle     16,779.28
    Total SM Elapsed Cycles          cycle   292,795,864
    Average SMSP Active Cycles       cycle     16,734.03
    Total SMSP Elapsed Cycles        cycle 1,171,183,456
    -------------------------- ----------- -------------

  Add(double *, double *, double) (128, 1, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         2.62
    SM Frequency            cycle/nsecond         1.52
    Elapsed Cycles                  cycle       20,522
    Memory Throughput                   %         7.43
    DRAM Throughput                     %         7.43
    Duration                      usecond        13.44
    L1/TEX Cache Throughput             %         6.93
    L2 Cache Throughput                 %        14.50
    SM Active Cycles                cycle    13,074.69
    Compute (SM) Throughput             %        13.87
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.2 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Threads                                   thread          65,536
    Uses Green Context                                             0
    Waves Per SM                                                0.24
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 3.03%                                                                                           
          The grid for this launch is configured to execute only 128 blocks, which is less than the GPU's 132           
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit SM                        block           32
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        24.83
    Achieved Active Warps Per SM           warp        15.89
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75.17%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     2,612.50
    Total DRAM Elapsed Cycles        cycle    1,687,552
    Average L1 Active Cycles         cycle    13,074.69
    Total L1 Elapsed Cycles          cycle    2,703,924
    Average L2 Active Cycles         cycle    12,000.74
    Total L2 Elapsed Cycles          cycle    2,185,152
    Average SM Active Cycles         cycle    13,074.69
    Total SM Elapsed Cycles          cycle    2,703,924
    Average SMSP Active Cycles       cycle    13,027.77
    Total SMSP Elapsed Cycles        cycle   10,815,696
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.186%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 8.13% above the average, while the minimum instance value is 100.00% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.578%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 11.91% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.186%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 8.13% above the average, while the minimum instance value is 100.00% below the      
          average.                                                                                                      

  SpatialFilter(double *, double *) (128, 1, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         2.62
    SM Frequency            cycle/nsecond         1.53
    Elapsed Cycles                  cycle      721,910
    Memory Throughput                   %         2.69
    DRAM Throughput                     %         0.11
    Duration                      usecond       471.87
    L1/TEX Cache Throughput             %         2.94
    L2 Cache Throughput                 %         1.29
    SM Active Cycles                cycle   659,847.10
    Compute (SM) Throughput             %        36.39
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 1.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              66
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Threads                                   thread          65,536
    Uses Green Context                                             0
    Waves Per SM                                                0.97
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 3.03%                                                                                           
          The grid for this launch is configured to execute only 128 blocks, which is less than the GPU's 132           
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit SM                        block           32
    Block Limit Registers                 block            1
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        24.60
    Achieved Active Warps Per SM           warp        15.74
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (25.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     1,318.33
    Total DRAM Elapsed Cycles        cycle   59,315,328
    Average L1 Active Cycles         cycle   659,847.10
    Total L1 Elapsed Cycles          cycle   95,266,830
    Average L2 Active Cycles         cycle   127,990.40
    Total L2 Elapsed Cycles          cycle   77,328,960
    Average SM Active Cycles         cycle   659,847.10
    Total SM Elapsed Cycles          cycle   95,266,830
    Average SMSP Active Cycles       cycle   654,115.61
    Total SMSP Elapsed Cycles        cycle  381,067,320
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.092%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 7.76% above the average, while the minimum instance value is 100.00% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.782%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 8.59% above the average, while the minimum instance value is 100.00% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.092%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 7.76% above the average, while the minimum instance value is 100.00% below the      
          average.                                                                                                      

  Pearson(double *, double *, double *) (1, 1, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         2.62
    SM Frequency            cycle/nsecond         1.53
    Elapsed Cycles                  cycle    2,218,318
    Memory Throughput                   %         0.07
    DRAM Throughput                     %         0.07
    Duration                      msecond         1.45
    L1/TEX Cache Throughput             %         4.52
    L2 Cache Throughput                 %         0.09
    SM Active Cycles                cycle    16,797.47
    Compute (SM) Throughput             %         0.21
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Threads                                   thread             512
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 99.24%                                                                                          
          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 132             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        24.93
    Achieved Active Warps Per SM           warp        15.95
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50.15%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (24.9%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      2,618.67
    Total DRAM Elapsed Cycles        cycle   182,267,904
    Average L1 Active Cycles         cycle     16,797.47
    Total L1 Elapsed Cycles          cycle   292,779,864
    Average L2 Active Cycles         cycle     98,842.17
    Total L2 Elapsed Cycles          cycle   237,748,800
    Average SM Active Cycles         cycle     16,797.47
    Total SM Elapsed Cycles          cycle   292,779,864
    Average SMSP Active Cycles       cycle     16,742.37
    Total SMSP Elapsed Cycles        cycle 1,171,119,456
    -------------------------- ----------- -------------

  Pearson(double *, double *, double *) (1, 1, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         2.62
    SM Frequency            cycle/nsecond         1.53
    Elapsed Cycles                  cycle    2,219,160
    Memory Throughput                   %         0.07
    DRAM Throughput                     %         0.07
    Duration                      msecond         1.45
    L1/TEX Cache Throughput             %         4.53
    L2 Cache Throughput                 %         0.09
    SM Active Cycles                cycle    16,780.58
    Compute (SM) Throughput             %         0.21
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Threads                                   thread             512
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 99.24%                                                                                          
          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 132             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        24.93
    Achieved Active Warps Per SM           warp        15.95
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50.14%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (24.9%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      2,618.83
    Total DRAM Elapsed Cycles        cycle   182,335,488
    Average L1 Active Cycles         cycle     16,780.58
    Total L1 Elapsed Cycles          cycle   292,868,852
    Average L2 Active Cycles         cycle     97,430.72
    Total L2 Elapsed Cycles          cycle   237,748,320
    Average SM Active Cycles         cycle     16,780.58
    Total SM Elapsed Cycles          cycle   292,868,852
    Average SMSP Active Cycles       cycle     16,727.78
    Total SMSP Elapsed Cycles        cycle 1,171,475,408
    -------------------------- ----------- -------------

  Add(double *, double *, double) (128, 1, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         2.61
    SM Frequency            cycle/nsecond         1.52
    Elapsed Cycles                  cycle       21,460
    Memory Throughput                   %         7.11
    DRAM Throughput                     %         7.11
    Duration                      usecond        14.08
    L1/TEX Cache Throughput             %         6.72
    L2 Cache Throughput                 %        13.77
    SM Active Cycles                cycle    13,484.11
    Compute (SM) Throughput             %        13.76
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.2 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Threads                                   thread          65,536
    Uses Green Context                                             0
    Waves Per SM                                                0.24
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 3.03%                                                                                           
          The grid for this launch is configured to execute only 128 blocks, which is less than the GPU's 132           
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit SM                        block           32
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        24.82
    Achieved Active Warps Per SM           warp        15.89
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75.18%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     2,613.50
    Total DRAM Elapsed Cycles        cycle    1,763,328
    Average L1 Active Cycles         cycle    13,484.11
    Total L1 Elapsed Cycles          cycle    2,827,306
    Average L2 Active Cycles         cycle    11,950.90
    Total L2 Elapsed Cycles          cycle    2,285,856
    Average SM Active Cycles         cycle    13,484.11
    Total SM Elapsed Cycles          cycle    2,827,306
    Average SMSP Active Cycles       cycle    13,412.02
    Total SMSP Elapsed Cycles        cycle   11,309,224
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.097%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 9.68% above the average, while the minimum instance value is 100.00% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.634%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 9.00% above the average, while the minimum instance value is 100.00% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.097%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 9.68% above the average, while the minimum instance value is 100.00% below the      
          average.                                                                                                      

  SpatialFilter(double *, double *) (128, 1, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         2.62
    SM Frequency            cycle/nsecond         1.53
    Elapsed Cycles                  cycle      715,145
    Memory Throughput                   %         2.72
    DRAM Throughput                     %         0.11
    Duration                      usecond       467.46
    L1/TEX Cache Throughput             %         2.98
    L2 Cache Throughput                 %         1.31
    SM Active Cycles                cycle   651,955.23
    Compute (SM) Throughput             %        36.27
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 1.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              66
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Threads                                   thread          65,536
    Uses Green Context                                             0
    Waves Per SM                                                0.97
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 3.03%                                                                                           
          The grid for this launch is configured to execute only 128 blocks, which is less than the GPU's 132           
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit SM                        block           32
    Block Limit Registers                 block            1
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        24.60
    Achieved Active Warps Per SM           warp        15.74
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (25.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     1,318.67
    Total DRAM Elapsed Cycles        cycle   58,760,448
    Average L1 Active Cycles         cycle   651,955.23
    Total L1 Elapsed Cycles          cycle   94,379,696
    Average L2 Active Cycles         cycle   139,851.72
    Total L2 Elapsed Cycles          cycle   76,580,064
    Average SM Active Cycles         cycle   651,955.23
    Total SM Elapsed Cycles          cycle   94,379,696
    Average SMSP Active Cycles       cycle   646,456.82
    Total SMSP Elapsed Cycles        cycle  377,518,784
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.351%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 8.06% above the average, while the minimum instance value is 100.00% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.959%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 8.80% above the average, while the minimum instance value is 100.00% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.351%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 8.06% above the average, while the minimum instance value is 100.00% below the      
          average.                                                                                                      

  Pearson(double *, double *, double *) (1, 1, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         2.62
    SM Frequency            cycle/nsecond         1.53
    Elapsed Cycles                  cycle    2,220,242
    Memory Throughput                   %         0.07
    DRAM Throughput                     %         0.07
    Duration                      msecond         1.45
    L1/TEX Cache Throughput             %         4.53
    L2 Cache Throughput                 %         0.09
    SM Active Cycles                cycle    16,761.52
    Compute (SM) Throughput             %         0.21
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Threads                                   thread             512
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 99.24%                                                                                          
          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 132             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        24.93
    Achieved Active Warps Per SM           warp        15.95
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50.14%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (24.9%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      2,619.17
    Total DRAM Elapsed Cycles        cycle   182,426,112
    Average L1 Active Cycles         cycle     16,761.52
    Total L1 Elapsed Cycles          cycle   293,016,982
    Average L2 Active Cycles         cycle     99,470.17
    Total L2 Elapsed Cycles          cycle   237,829,824
    Average SM Active Cycles         cycle     16,761.52
    Total SM Elapsed Cycles          cycle   293,016,982
    Average SMSP Active Cycles       cycle     16,746.88
    Total SMSP Elapsed Cycles        cycle 1,172,067,928
    -------------------------- ----------- -------------

  Pearson(double *, double *, double *) (1, 1, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         2.62
    SM Frequency            cycle/nsecond         1.53
    Elapsed Cycles                  cycle    2,217,666
    Memory Throughput                   %         0.07
    DRAM Throughput                     %         0.07
    Duration                      msecond         1.45
    L1/TEX Cache Throughput             %         4.53
    L2 Cache Throughput                 %         0.07
    SM Active Cycles                cycle    16,776.47
    Compute (SM) Throughput             %         0.21
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Threads                                   thread             512
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 99.24%                                                                                          
          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 132             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        24.93
    Achieved Active Warps Per SM           warp        15.95
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50.15%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (24.9%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      2,618.17
    Total DRAM Elapsed Cycles        cycle   182,212,864
    Average L1 Active Cycles         cycle     16,776.47
    Total L1 Elapsed Cycles          cycle   292,711,296
    Average L2 Active Cycles         cycle     95,713.32
    Total L2 Elapsed Cycles          cycle   237,693,696
    Average SM Active Cycles         cycle     16,776.47
    Total SM Elapsed Cycles          cycle   292,711,296
    Average SMSP Active Cycles       cycle     16,728.15
    Total SMSP Elapsed Cycles        cycle 1,170,845,184
    -------------------------- ----------- -------------

  Add(double *, double *, double) (128, 1, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         2.61
    SM Frequency            cycle/nsecond         1.52
    Elapsed Cycles                  cycle       20,619
    Memory Throughput                   %         7.40
    DRAM Throughput                     %         7.40
    Duration                      usecond        13.50
    L1/TEX Cache Throughput             %         6.71
    L2 Cache Throughput                 %        14.28
    SM Active Cycles                cycle    13,524.52
    Compute (SM) Throughput             %        14.32
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.2 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Threads                                   thread          65,536
    Uses Green Context                                             0
    Waves Per SM                                                0.24
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 3.03%                                                                                           
          The grid for this launch is configured to execute only 128 blocks, which is less than the GPU's 132           
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit SM                        block           32
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        24.76
    Achieved Active Warps Per SM           warp        15.85
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75.24%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     2,612.83
    Total DRAM Elapsed Cycles        cycle    1,693,696
    Average L1 Active Cycles         cycle    13,524.52
    Total L1 Elapsed Cycles          cycle    2,715,840
    Average L2 Active Cycles         cycle    12,040.57
    Total L2 Elapsed Cycles          cycle    2,195,424
    Average SM Active Cycles         cycle    13,524.52
    Total SM Elapsed Cycles          cycle    2,715,840
    Average SMSP Active Cycles       cycle    13,387.48
    Total SMSP Elapsed Cycles        cycle   10,863,360
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.667%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 10.14% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.226%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 9.57% above the average, while the minimum instance value is 100.00% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.667%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 10.14% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.082%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 11.55% above the average, while the minimum instance value is 9.31% below the       
          average.                                                                                                      

  SpatialFilter(double *, double *) (128, 1, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         2.62
    SM Frequency            cycle/nsecond         1.53
    Elapsed Cycles                  cycle      715,435
    Memory Throughput                   %         2.71
    DRAM Throughput                     %         0.11
    Duration                      usecond       467.65
    L1/TEX Cache Throughput             %         2.97
    L2 Cache Throughput                 %         1.31
    SM Active Cycles                cycle   652,730.57
    Compute (SM) Throughput             %        36.27
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 1.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              66
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Threads                                   thread          65,536
    Uses Green Context                                             0
    Waves Per SM                                                0.97
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 3.03%                                                                                           
          The grid for this launch is configured to execute only 128 blocks, which is less than the GPU's 132           
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit SM                        block           32
    Block Limit Registers                 block            1
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        24.58
    Achieved Active Warps Per SM           warp        15.73
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (25.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     1,318.17
    Total DRAM Elapsed Cycles        cycle   58,785,280
    Average L1 Active Cycles         cycle   652,730.57
    Total L1 Elapsed Cycles          cycle   94,411,166
    Average L2 Active Cycles         cycle   142,077.31
    Total L2 Elapsed Cycles          cycle   76,619,808
    Average SM Active Cycles         cycle   652,730.57
    Total SM Elapsed Cycles          cycle   94,411,166
    Average SMSP Active Cycles       cycle   646,923.66
    Total SMSP Elapsed Cycles        cycle  377,644,664
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.397%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 8.10% above the average, while the minimum instance value is 100.00% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.063%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 8.91% above the average, while the minimum instance value is 100.00% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.397%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 8.10% above the average, while the minimum instance value is 100.00% below the      
          average.                                                                                                      

  Pearson(double *, double *, double *) (1, 1, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         2.62
    SM Frequency            cycle/nsecond         1.53
    Elapsed Cycles                  cycle    2,220,382
    Memory Throughput                   %         0.07
    DRAM Throughput                     %         0.07
    Duration                      msecond         1.45
    L1/TEX Cache Throughput             %         4.53
    L2 Cache Throughput                 %         0.07
    SM Active Cycles                cycle    16,783.25
    Compute (SM) Throughput             %         0.21
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Threads                                   thread             512
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 99.24%                                                                                          
          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 132             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        24.93
    Achieved Active Warps Per SM           warp        15.95
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50.14%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (24.9%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      2,618.33
    Total DRAM Elapsed Cycles        cycle   182,434,816
    Average L1 Active Cycles         cycle     16,783.25
    Total L1 Elapsed Cycles          cycle   293,055,788
    Average L2 Active Cycles         cycle    104,644.92
    Total L2 Elapsed Cycles          cycle   237,976,800
    Average SM Active Cycles         cycle     16,783.25
    Total SM Elapsed Cycles          cycle   293,055,788
    Average SMSP Active Cycles       cycle     16,746.80
    Total SMSP Elapsed Cycles        cycle 1,172,223,152
    -------------------------- ----------- -------------

